{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "name": "new_tabnet(tpu_multicore_version).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sugangnb/ai-research/blob/main/new_tabnet(tpu_multicore_version).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1jxRBdC4A719"
      },
      "source": [
        "import os\n",
        "assert os.environ['COLAB_TPU_ADDR'], 'Make sure to select TPU from Edit > Notebook settings > Hardware accelerator'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PsKsFirmVPVo",
        "outputId": "b52f06fb-db85-47af-8d40-598e2eccfdbd"
      },
      "source": [
        "!rm -rf kicked_dataset/\r\n",
        "!git clone https://github.com/calibertytz/kicked_dataset.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'kicked_dataset'...\n",
            "remote: Enumerating objects: 3, done.\u001b[K\n",
            "remote: Counting objects: 100% (3/3), done.\u001b[K\n",
            "remote: Compressing objects: 100% (3/3), done.\u001b[K\n",
            "remote: Total 25 (delta 0), reused 0 (delta 0), pack-reused 22\u001b[K\n",
            "Unpacking objects: 100% (25/25), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RCxJ4WBlASfq",
        "outputId": "13f4e3a3-22b5-4b13-f72c-0fb48c3b226d"
      },
      "source": [
        "VERSION = \"nightly\"  #@param [\"1.5\" , \"20200325\", \"nightly\"]\n",
        "!curl https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py\n",
        "!python pytorch-xla-env-setup.py --version $VERSION"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r100  5116  100  5116    0     0  32585      0 --:--:-- --:--:-- --:--:-- 32585\n",
            "Updating... This may take around 2 minutes.\n",
            "Updating TPU runtime to pytorch-nightly ...\n",
            "Collecting cloud-tpu-client\n",
            "  Downloading https://files.pythonhosted.org/packages/56/9f/7b1958c2886db06feb5de5b2c191096f9e619914b6c31fdf93999fdbbd8b/cloud_tpu_client-0.10-py3-none-any.whl\n",
            "Requirement already satisfied: oauth2client in /usr/local/lib/python3.6/dist-packages (from cloud-tpu-client) (4.1.3)\n",
            "Collecting google-api-python-client==1.8.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9a/b4/a955f393b838bc47cbb6ae4643b9d0f90333d3b4db4dc1e819f36aad18cc/google_api_python_client-1.8.0-py3-none-any.whl (57kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 3.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: httplib2>=0.9.1 in /usr/local/lib/python3.6/dist-packages (from oauth2client->cloud-tpu-client) (0.17.4)\n",
            "Requirement already satisfied: six>=1.6.1 in /usr/local/lib/python3.6/dist-packages (from oauth2client->cloud-tpu-client) (1.15.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.6/dist-packages (from oauth2client->cloud-tpu-client) (0.2.8)\n",
            "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.6/dist-packages (from oauth2client->cloud-tpu-client) (0.4.8)\n",
            "Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from oauth2client->cloud-tpu-client) (4.6)\n",
            "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client) (3.0.1)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client) (0.0.4)\n",
            "Requirement already satisfied: google-api-core<2dev,>=1.13.0 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client) (1.16.0)\n",
            "Requirement already satisfied: google-auth>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client) (1.17.2)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.6/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client) (2018.9)\n",
            "Requirement already satisfied: protobuf>=3.4.0 in /usr/local/lib/python3.6/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client) (3.12.4)\n",
            "Requirement already satisfied: setuptools>=34.0.0 in /usr/local/lib/python3.6/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client) (51.1.1)\n",
            "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.6/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client) (2.23.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client) (1.52.0)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.4.1->google-api-python-client==1.8.0->cloud-tpu-client) (4.2.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client) (2020.12.5)\n",
            "Uninstalling torch-1.7.0+cu101:\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client) (1.24.3)\n",
            "Installing collected packages: google-api-python-client, cloud-tpu-client\n",
            "  Found existing installation: google-api-python-client 1.7.12\n",
            "    Uninstalling google-api-python-client-1.7.12:\n",
            "      Successfully uninstalled google-api-python-client-1.7.12\n",
            "Successfully installed cloud-tpu-client-0.10 google-api-python-client-1.8.0\n",
            "  Successfully uninstalled torch-1.7.0+cu101\n",
            "Uninstalling torchvision-0.8.1+cu101:\n",
            "  Successfully uninstalled torchvision-0.8.1+cu101\n",
            "Copying gs://tpu-pytorch/wheels/torch-nightly-cp36-cp36m-linux_x86_64.whl...\n",
            "\\\n",
            "Operation completed over 1 objects/122.0 MiB.                                    \n",
            "Copying gs://tpu-pytorch/wheels/torch_xla-nightly-cp36-cp36m-linux_x86_64.whl...\n",
            "\\\n",
            "Operation completed over 1 objects/130.8 MiB.                                    \n",
            "Copying gs://tpu-pytorch/wheels/torchvision-nightly-cp36-cp36m-linux_x86_64.whl...\n",
            "/ [1 files][  4.8 MiB/  4.8 MiB]                                                \n",
            "Operation completed over 1 objects/4.8 MiB.                                      \n",
            "Done updating TPU runtime\n",
            "Processing ./torch-nightly-cp36-cp36m-linux_x86_64.whl\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch==nightly) (1.19.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch==nightly) (3.7.4.3)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from torch==nightly) (0.8)\n",
            "\u001b[31mERROR: fastai 1.0.61 requires torchvision, which is not installed.\u001b[0m\n",
            "Installing collected packages: torch\n",
            "Successfully installed torch-1.8.0a0\n",
            "Processing ./torch_xla-nightly-cp36-cp36m-linux_x86_64.whl\n",
            "Installing collected packages: torch-xla\n",
            "Successfully installed torch-xla-1.6+ab2ceda\n",
            "Processing ./torchvision-nightly-cp36-cp36m-linux_x86_64.whl\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision==nightly) (1.19.4)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from torchvision==nightly) (1.8.0a0)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision==nightly) (7.0.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch->torchvision==nightly) (3.7.4.3)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from torch->torchvision==nightly) (0.8)\n",
            "Installing collected packages: torchvision\n",
            "Successfully installed torchvision-0.9.0a0+3ee34eb\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  libomp5\n",
            "0 upgraded, 1 newly installed, 0 to remove and 16 not upgraded.\n",
            "Need to get 234 kB of archives.\n",
            "After this operation, 774 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libomp5 amd64 5.0.1-1 [234 kB]\n",
            "Fetched 234 kB in 1s (329 kB/s)\n",
            "Selecting previously unselected package libomp5:amd64.\n",
            "(Reading database ... 145483 files and directories currently installed.)\n",
            "Preparing to unpack .../libomp5_5.0.1-1_amd64.deb ...\n",
            "Unpacking libomp5:amd64 (5.0.1-1) ...\n",
            "Setting up libomp5:amd64 (5.0.1-1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1.2) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.6/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oEj05oHzVgSq"
      },
      "source": [
        "import pandas as pd\r\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lp-ntRjRWII0"
      },
      "source": [
        "df_train = pd.read_csv('kicked_dataset/df_train_final.csv')\r\n",
        "df_test = pd.read_csv('kicked_dataset/df_test_final.csv')\r\n",
        "\r\n",
        "X = df_train.drop(columns=['label']).values\r\n",
        "y = df_train['label'].values\r\n",
        "\r\n",
        "X_test = df_test.drop(columns=['label']).values\r\n",
        "y_test = df_test['label'].values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bLM-BLDdZrQB"
      },
      "source": [
        "# Attention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ihum7X038fM-"
      },
      "source": [
        "from torch import nn\r\n",
        "from torch.autograd import Function\r\n",
        "import torch.nn.functional as F\r\n",
        "import torch\r\n",
        "import torch_xla.core.xla_model as xm\r\n",
        "\r\n",
        "class EntmaxBisectFunction(Function):\r\n",
        "    @classmethod\r\n",
        "    def _gp(cls, x, alpha):\r\n",
        "        return x ** (alpha - 1)\r\n",
        "\r\n",
        "    @classmethod\r\n",
        "    def _gp_inv(cls, y, alpha):\r\n",
        "\r\n",
        "        return y ** (1 / (alpha - 1))\r\n",
        "\r\n",
        "    @classmethod\r\n",
        "    def _p(cls, X, alpha):\r\n",
        "        return cls._gp_inv(torch.clamp(X, min=0), alpha)\r\n",
        "\r\n",
        "    @classmethod\r\n",
        "    def forward(cls, ctx, X, alpha=1.5, dim=-1, n_iter=50, ensure_sum_one=True):\r\n",
        "        \r\n",
        "        device = X.device\r\n",
        "        if not isinstance(alpha, torch.Tensor):\r\n",
        "            alpha = torch.tensor(alpha, dtype=X.dtype, device=device)\r\n",
        "        \r\n",
        "        if not xm.is_xla_tensor(alpha):\r\n",
        "            alpha = torch.tensor(alpha, dtype=X.dtype, device=device)\r\n",
        "\r\n",
        "        alpha_shape = list(X.shape)\r\n",
        "        alpha_shape[dim] = 1\r\n",
        "        alpha = alpha.expand(*alpha_shape)\r\n",
        "        ctx.alpha = alpha\r\n",
        "        ctx.dim = dim\r\n",
        "        d = X.shape[dim]\r\n",
        "        X = X * (alpha - 1)\r\n",
        "\r\n",
        "        max_val, _ = X.max(dim=dim, keepdim=True)\r\n",
        "\r\n",
        "        tau_lo = max_val - cls._gp(1, alpha)\r\n",
        "        tau_hi = max_val - cls._gp(1 / d, alpha)\r\n",
        "        f_lo = cls._p(X - tau_lo, alpha).sum(dim) - 1\r\n",
        "        dm = tau_hi - tau_lo\r\n",
        "        for it in range(n_iter):\r\n",
        "\r\n",
        "            dm /= 2\r\n",
        "            tau_m = tau_lo + dm\r\n",
        "            p_m = cls._p(X - tau_m, alpha)\r\n",
        "            f_m = p_m.sum(dim) - 1\r\n",
        "\r\n",
        "            mask = (f_m * f_lo >= 0).unsqueeze(dim)\r\n",
        "            tau_lo = torch.where(mask, tau_m, tau_lo)\r\n",
        "\r\n",
        "        if ensure_sum_one:\r\n",
        "            p_m /= p_m.sum(dim=dim).unsqueeze(dim=dim)\r\n",
        "        \r\n",
        "        ctx.save_for_backward(p_m)\r\n",
        "        \r\n",
        "        return p_m\r\n",
        "\r\n",
        "    @classmethod\r\n",
        "    def backward(cls, ctx, dY):\r\n",
        "        Y, = ctx.saved_tensors\r\n",
        "\r\n",
        "        gppr = torch.where(Y > 0, Y ** (2 - ctx.alpha), Y.new_zeros(1))\r\n",
        "\r\n",
        "        dX = dY * gppr\r\n",
        "        q = dX.sum(ctx.dim) / gppr.sum(ctx.dim)\r\n",
        "        q = q.unsqueeze(ctx.dim)\r\n",
        "        dX -= q * gppr\r\n",
        "\r\n",
        "        d_alpha = None\r\n",
        "        if ctx.needs_input_grad[1]:\r\n",
        "\r\n",
        "            # alpha gradient computation\r\n",
        "            # d_alpha = (partial_y / partial_alpha) * dY\r\n",
        "            # NOTE: ensure alpha is not close to 1\r\n",
        "            # since there is an indetermination\r\n",
        "            # batch_size, _ = dY.shape\r\n",
        "\r\n",
        "            # shannon terms\r\n",
        "            S = torch.where(Y > 0, - Y * torch.log(Y), Y.new_zeros(1))\r\n",
        "\r\n",
        "            # shannon entropy\r\n",
        "            ent = S.sum(ctx.dim).unsqueeze(ctx.dim)\r\n",
        "            Y_skewed = gppr / gppr.sum(ctx.dim).unsqueeze(ctx.dim)\r\n",
        "\r\n",
        "            d_alpha = dY * (Y - Y_skewed) / ((ctx.alpha - 1) ** 2)\r\n",
        "            d_alpha += dY * (S - Y_skewed * ent) / (ctx.alpha - 1)\r\n",
        "            d_alpha = d_alpha.sum(ctx.dim).unsqueeze(ctx.dim)\r\n",
        "\r\n",
        "        return dX, d_alpha, None, None, None\r\n",
        "\r\n",
        "\r\n",
        "# slightly more efficient special case for sparsemax\r\n",
        "class SparsemaxBisectFunction(EntmaxBisectFunction):\r\n",
        "    @classmethod\r\n",
        "    def _gp(cls, x, alpha):\r\n",
        "        return x\r\n",
        "\r\n",
        "    @classmethod\r\n",
        "    def _gp_inv(cls, y, alpha):\r\n",
        "        return y\r\n",
        "\r\n",
        "    @classmethod\r\n",
        "    def _p(cls, x, alpha):\r\n",
        "        return torch.clamp(x, min=0)\r\n",
        "\r\n",
        "    @classmethod\r\n",
        "    def forward(cls, ctx, X, dim=-1, n_iter=50, ensure_sum_one=True):\r\n",
        "        return super().forward(\r\n",
        "            ctx, X, alpha=2, dim=dim, n_iter=50, ensure_sum_one=True\r\n",
        "        )\r\n",
        "\r\n",
        "    @classmethod\r\n",
        "    def backward(cls, ctx, dY):\r\n",
        "        Y, = ctx.saved_tensors\r\n",
        "        gppr = (Y > 0).to(dtype=dY.dtype)\r\n",
        "        dX = dY * gppr\r\n",
        "        q = dX.sum(ctx.dim) / gppr.sum(ctx.dim)\r\n",
        "        q = q.unsqueeze(ctx.dim)\r\n",
        "        dX -= q * gppr\r\n",
        "        return dX, None, None, None\r\n",
        "\r\n",
        "\r\n",
        "def entmax_bisect(X, alpha=1.5, dim=-1, n_iter=50, ensure_sum_one=True):\r\n",
        "    \"\"\"alpha-entmax: normalizing sparse transform (a la softmax).\r\n",
        "    Solves the optimization problem:\r\n",
        "        max_p <x, p> - H_a(p)    s.t.    p >= 0, sum(p) == 1.\r\n",
        "    where H_a(p) is the Tsallis alpha-entropy with custom alpha >= 1,\r\n",
        "    using a bisection (root finding, binary search) algorithm.\r\n",
        "    This function is differentiable with respect to both X and alpha.\r\n",
        "    Parameters\r\n",
        "    ----------\r\n",
        "    X : torch.Tensor\r\n",
        "        The input tensor.\r\n",
        "    alpha : float or torch.Tensor\r\n",
        "        Tensor of alpha parameters (> 1) to use. If scalar\r\n",
        "        or python float, the same value is used for all rows, otherwise,\r\n",
        "        it must have shape (or be expandable to)\r\n",
        "        alpha.shape[j] == (X.shape[j] if j != dim else 1)\r\n",
        "        A value of alpha=2 corresponds to sparsemax, and alpha=1 corresponds to\r\n",
        "        softmax (but computing it this way is likely unstable).\r\n",
        "    dim : int\r\n",
        "        The dimension along which to apply alpha-entmax.\r\n",
        "    n_iter : int\r\n",
        "        Number of bisection iterations. For float32, 24 iterations should\r\n",
        "        suffice for machine precision.\r\n",
        "    ensure_sum_one : bool,\r\n",
        "        Whether to divide the result by its sum. If false, the result might\r\n",
        "        sum to close but not exactly 1, which might cause downstream problems.\r\n",
        "    Returns\r\n",
        "    -------\r\n",
        "    P : torch tensor, same shape as X\r\n",
        "        The projection result, such that P.sum(dim=dim) == 1 elementwise.\r\n",
        "    \"\"\"\r\n",
        "    return EntmaxBisectFunction.apply(X, alpha, dim, n_iter, ensure_sum_one)\r\n",
        "\r\n",
        "\r\n",
        "def sparsemax_bisect(X, dim=-1, n_iter=50, ensure_sum_one=True):\r\n",
        "    \"\"\"sparsemax: normalizing sparse transform (a la softmax), via bisection.\r\n",
        "    Solves the projection:\r\n",
        "        min_p ||x - p||_2   s.t.    p >= 0, sum(p) == 1.\r\n",
        "    Parameters\r\n",
        "    ----------\r\n",
        "    X : torch.Tensor\r\n",
        "        The input tensor.\r\n",
        "    dim : int\r\n",
        "        The dimension along which to apply sparsemax.\r\n",
        "    n_iter : int\r\n",
        "        Number of bisection iterations. For float32, 24 iterations should\r\n",
        "        suffice for machine precision.\r\n",
        "    ensure_sum_one : bool,\r\n",
        "        Whether to divide the result by its sum. If false, the result might\r\n",
        "        sum to close but not exactly 1, which might cause downstream problems.\r\n",
        "    Note: This function does not yet support normalizing along anything except\r\n",
        "    the last dimension. Please use transposing and views to achieve more\r\n",
        "    general behavior.\r\n",
        "    Returns\r\n",
        "    -------\r\n",
        "    P : torch tensor, same shape as X\r\n",
        "        The projection result, such that P.sum(dim=dim) == 1 elementwise.\r\n",
        "    \"\"\"\r\n",
        "    return SparsemaxBisectFunction.apply(X, dim, n_iter, ensure_sum_one)\r\n",
        "\r\n",
        "\r\n",
        "class SparsemaxBisect(nn.Module):\r\n",
        "    def __init__(self, dim=-1, n_iter=None):\r\n",
        "        \"\"\"sparsemax: normalizing sparse transform (a la softmax) via bisection\r\n",
        "        Solves the projection:\r\n",
        "            min_p ||x - p||_2   s.t.    p >= 0, sum(p) == 1.\r\n",
        "        Parameters\r\n",
        "        ----------\r\n",
        "        dim : int\r\n",
        "            The dimension along which to apply sparsemax.\r\n",
        "        n_iter : int\r\n",
        "            Number of bisection iterations. For float32, 24 iterations should\r\n",
        "            suffice for machine precision.\r\n",
        "        \"\"\"\r\n",
        "        self.dim = dim\r\n",
        "        self.n_iter = n_iter\r\n",
        "        super().__init__()\r\n",
        "\r\n",
        "    def forward(self, X):\r\n",
        "        return sparsemax_bisect(X, dim=self.dim, n_iter=self.n_iter)\r\n",
        "\r\n",
        "\r\n",
        "class EntmaxBisect(nn.Module):\r\n",
        "    def __init__(self, alpha=1.8, dim=-1, n_iter=50):\r\n",
        "        \"\"\"alpha-entmax: normalizing sparse map (a la softmax) via bisection.\r\n",
        "        Solves the optimization problem:\r\n",
        "            max_p <x, p> - H_a(p)    s.t.    p >= 0, sum(p) == 1.\r\n",
        "        where H_a(p) is the Tsallis alpha-entropy with custom alpha >= 1,\r\n",
        "        using a bisection (root finding, binary search) algorithm.\r\n",
        "        Parameters\r\n",
        "        ----------\r\n",
        "        alpha : float or torch.Tensor\r\n",
        "            Tensor of alpha parameters (> 1) to use. If scalar\r\n",
        "            or python float, the same value is used for all rows, otherwise,\r\n",
        "            it must have shape (or be expandable to)\r\n",
        "            alpha.shape[j] == (X.shape[j] if j != dim else 1)\r\n",
        "            A value of alpha=2 corresponds to sparsemax; alpha=1 corresponds\r\n",
        "            to softmax (but computing it this way is likely unstable).\r\n",
        "        dim : int\r\n",
        "            The dimension along which to apply alpha-entmax.\r\n",
        "        n_iter : int\r\n",
        "            Number of bisection iterations. For float32, 24 iterations should\r\n",
        "            suffice for machine precision.\r\n",
        "        \"\"\"\r\n",
        "        self.dim = dim\r\n",
        "        self.n_iter = n_iter\r\n",
        "        self.alpha = alpha\r\n",
        "        super().__init__()\r\n",
        "\r\n",
        "    def forward(self, X):\r\n",
        "        return entmax_bisect(\r\n",
        "            X, alpha=self.alpha, dim=self.dim, n_iter=self.n_iter\r\n",
        "        )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2hoi1JagZxsH"
      },
      "source": [
        "import math\r\n",
        "\r\n",
        "\r\n",
        "class Attention(nn.Module):\r\n",
        "    \"\"\"\r\n",
        "    Compute 'Scaled Dot Product Attention\r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    def __init__(self, alpha=1.5):\r\n",
        "        super(Attention, self).__init__()\r\n",
        "        self.alpha = alpha * nn.Parameter(torch.ones([1]))\r\n",
        "        self.activation = EntmaxBisect(alpha=self.alpha, dim=-1)\r\n",
        "\r\n",
        "    def forward(self, query, key, value, mask=None, dropout=None):\r\n",
        "        scores = torch.matmul(query, key.transpose(-2, -1)) \\\r\n",
        "                 / math.sqrt(query.size(-1))\r\n",
        "\r\n",
        "        if mask is not None:\r\n",
        "            scores = scores.masked_fill(mask == 0, -1e9)\r\n",
        "\r\n",
        "        p_attn = self.activation(scores)\r\n",
        "\r\n",
        "        if dropout is not None:\r\n",
        "            p_attn = dropout(p_attn)\r\n",
        "\r\n",
        "        return torch.matmul(p_attn, value), p_attn\r\n",
        "\r\n",
        "class MultiHeadedAttention(nn.Module):\r\n",
        "    \"\"\"\r\n",
        "    Take in model size and number of heads.\r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    def __init__(self, h, d_model, dropout=0.1):\r\n",
        "        super().__init__()\r\n",
        "        assert d_model % h == 0\r\n",
        "\r\n",
        "        # We assume d_v always equals d_k\r\n",
        "        self.d_k = d_model // h\r\n",
        "        self.h = h\r\n",
        "\r\n",
        "        self.linear_layers = nn.ModuleList([nn.Linear(d_model, d_model) for _ in range(3)])\r\n",
        "        self.output_linear = nn.Linear(d_model, d_model)\r\n",
        "        self.attention = Attention()\r\n",
        "\r\n",
        "        self.dropout = nn.Dropout(p=dropout)\r\n",
        "\r\n",
        "    def forward(self, query, key, value, mask=None):\r\n",
        "        batch_size = query.size(0)\r\n",
        "\r\n",
        "        # 1) Do all the linear projections in batch from d_model => h x d_k\r\n",
        "        query, key, value = [l(x).view(batch_size, -1, self.h, self.d_k).transpose(1, 2)\r\n",
        "                             for l, x in zip(self.linear_layers, (query, key, value))]\r\n",
        "\r\n",
        "        # 2) Apply attention on all the projected vectors in batch.\r\n",
        "        x, attn = self.attention(query, key, value, mask=mask, dropout=self.dropout)\r\n",
        "\r\n",
        "        # 3) \"Concat\" using a view and apply a final linear.\r\n",
        "        x = x.transpose(1, 2).contiguous().view(batch_size, -1, self.h * self.d_k)\r\n",
        "\r\n",
        "        return self.output_linear(x)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "idN91fFvZ96M"
      },
      "source": [
        "# utils"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C4gyFGumZ9Qp"
      },
      "source": [
        "import torch.nn as nn\r\n",
        "import torch\r\n",
        "\r\n",
        "class PositionwiseFeedForward(nn.Module):\r\n",
        "    \"Implements FFN equation.\"\r\n",
        "    def __init__(self, d_model, d_ff, dropout=0.1):\r\n",
        "        super(PositionwiseFeedForward, self).__init__()\r\n",
        "        self.w_1 = nn.Linear(d_model, d_ff)\r\n",
        "        self.w_2 = nn.Linear(d_ff, d_model)\r\n",
        "        self.dropout = nn.Dropout(dropout)\r\n",
        "        self.activation = nn.GELU()\r\n",
        "\r\n",
        "    def forward(self, x):\r\n",
        "        return self.w_2(self.dropout(self.activation(self.w_1(x))))\r\n",
        "\r\n",
        "\r\n",
        "class LayerNorm(nn.Module):\r\n",
        "    \"Construct a layernorm module (See citation for details).\"\r\n",
        "\r\n",
        "    def __init__(self, features, eps=1e-12):\r\n",
        "        super(LayerNorm, self).__init__()\r\n",
        "        self.a_2 = nn.Parameter(torch.ones(features))\r\n",
        "        self.b_2 = nn.Parameter(torch.zeros(features))\r\n",
        "        self.eps = eps\r\n",
        "\r\n",
        "    def forward(self, x):\r\n",
        "        mean = x.mean(-1, keepdim=True)\r\n",
        "        std = x.std(-1, keepdim=True)\r\n",
        "        return self.a_2 * (x - mean) / (std + self.eps) + self.b_2\r\n",
        "\r\n",
        "class SublayerConnection(nn.Module):\r\n",
        "    \"\"\"\r\n",
        "    A residual connection followed by a layer norm.\r\n",
        "    Note for code simplicity the norm is first as opposed to last.\r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    def __init__(self, size, dropout):\r\n",
        "        super(SublayerConnection, self).__init__()\r\n",
        "        self.norm = LayerNorm(size)\r\n",
        "        self.dropout = nn.Dropout(dropout)\r\n",
        "\r\n",
        "    def forward(self, x, sublayer):\r\n",
        "        \"Apply residual connection to any sublayer with the same size.\"\r\n",
        "        return x + self.dropout(sublayer(self.norm(x)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "txom5RSjaN1C"
      },
      "source": [
        "# transformer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4kIoEh9jaMyR"
      },
      "source": [
        "class TransformerBlock(nn.Module):\r\n",
        "    \"\"\"\r\n",
        "    Bidirectional Encoder = Transformer (self-attention)\r\n",
        "    Transformer = MultiHead_Attention + Feed_Forward with sublayer connection\r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    def __init__(self, hidden, attn_heads, feed_forward_hidden, dropout):\r\n",
        "        \"\"\"\r\n",
        "        :param hidden: hidden size of transformer\r\n",
        "        :param attn_heads: head sizes of multi-head attention\r\n",
        "        :param feed_forward_hidden: feed_forward_hidden, usually 4*hidden_size\r\n",
        "        :param dropout: dropout rate\r\n",
        "        \"\"\"\r\n",
        "\r\n",
        "        super().__init__()\r\n",
        "        self.attention = MultiHeadedAttention(h=attn_heads, d_model=hidden)\r\n",
        "        self.feed_forward = PositionwiseFeedForward(d_model=hidden, d_ff=feed_forward_hidden, dropout=dropout)\r\n",
        "        self.input_sublayer = SublayerConnection(size=hidden, dropout=dropout)\r\n",
        "        self.output_sublayer = SublayerConnection(size=hidden, dropout=dropout)\r\n",
        "        self.dropout = nn.Dropout(p=dropout)\r\n",
        "\r\n",
        "    def forward(self, x, mask):\r\n",
        "        x = self.input_sublayer(x, lambda _x: self.attention.forward(_x, _x, _x, mask=mask))\r\n",
        "        x = self.output_sublayer(x, self.feed_forward)\r\n",
        "        return self.dropout(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J1fnJFLCaSo5"
      },
      "source": [
        "# model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hMEuRynKaT51"
      },
      "source": [
        "import torch.nn.functional as F\r\n",
        "import torch_xla.core.xla_model as xm\r\n",
        "\r\n",
        "\r\n",
        "class FeedForwardBlock(nn.Module):\r\n",
        "    '''\r\n",
        "    use rezero\r\n",
        "    '''\r\n",
        "\r\n",
        "    def __init__(self, in_dim, out_dim, dim_feedforward=2048, dropout=0.1, activation='relu'):\r\n",
        "        super(FeedForwardBlock, self).__init__()\r\n",
        "        self.linear1 = nn.Linear(in_dim, dim_feedforward)\r\n",
        "        self.dropout1 = nn.Dropout(dropout)\r\n",
        "        self.dropout2 = nn.Dropout(dropout)\r\n",
        "        self.linear2 = nn.Linear(dim_feedforward, out_dim)\r\n",
        "        self.resweight = nn.Parameter(torch.Tensor([0]))\r\n",
        "\r\n",
        "        if activation == \"relu\":\r\n",
        "            self.activation = F.relu\r\n",
        "        elif activation == \"gelu\":\r\n",
        "            self.activation = F.gelu\r\n",
        "\r\n",
        "    def forward(self, src):\r\n",
        "        src2 = src\r\n",
        "        src2 = self.linear2(self.dropout1(self.activation(self.linear1(src2))))\r\n",
        "        src2 = src2 * self.resweight\r\n",
        "        src = src + self.dropout2(src2)\r\n",
        "\r\n",
        "        return src\r\n",
        "\r\n",
        "\r\n",
        "class Encoder(nn.Module):\r\n",
        "    def __init__(self,\r\n",
        "                 input_dim,\r\n",
        "                 output_dim,\r\n",
        "                 hidden_dim,\r\n",
        "                 n_layers,\r\n",
        "                 attn_heads,\r\n",
        "                 dropout=0.1):\r\n",
        "\r\n",
        "        super().__init__()\r\n",
        "        self.input_dim = input_dim\r\n",
        "        self.output_dim = output_dim\r\n",
        "        self.hidden_dim = hidden_dim\r\n",
        "        self.n_layers = n_layers\r\n",
        "        self.attn_heads = attn_heads\r\n",
        "        self.dropout = dropout\r\n",
        "\r\n",
        "        self.embedding = nn.Linear(input_dim, hidden_dim)  # TODO: add attentive embedding\r\n",
        "\r\n",
        "        self.shared_base_block = TransformerBlock(self.hidden_dim, self.attn_heads, self.hidden_dim * 4, self.dropout)\r\n",
        "\r\n",
        "        self.shared_layer_tail = nn.ModuleList([FeedForwardBlock(in_dim=self.hidden_dim, out_dim=self.hidden_dim) for _\r\n",
        "                                                in range(int(self.n_layers / 2))])\r\n",
        "\r\n",
        "        self.no_shared_layer_head = nn.ModuleList(\r\n",
        "            TransformerBlock(self.hidden_dim, self.attn_heads, self.hidden_dim * 4, self.dropout) for _\r\n",
        "            in range(int(self.n_layers / 2)))\r\n",
        "        self.no_shared_layer_tail = nn.ModuleList(\r\n",
        "            FeedForwardBlock(in_dim=self.hidden_dim, out_dim=self.hidden_dim) for _\r\n",
        "            in range(int(self.n_layers / 2)))\r\n",
        "\r\n",
        "        self.final_layer = nn.Sequential(nn.Sigmoid(),\r\n",
        "                                         nn.Linear(self.hidden_dim, self.output_dim))\r\n",
        "\r\n",
        "    def forward(self, x):\r\n",
        "        x = self.embedding(x)\r\n",
        "        for i in range(self.n_layers):\r\n",
        "            if i % 2 != 0:\r\n",
        "                x = self.shared_base_block(x, mask=None)\r\n",
        "                x = self.shared_layer_tail[int(i / 2)](x)\r\n",
        "            else:\r\n",
        "                x = self.no_shared_layer_head[int(i / 2)](x, mask=None)\r\n",
        "                x = self.no_shared_layer_tail[int(i / 2)](x)\r\n",
        "        return self.final_layer(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rSBdCMX9BN2c"
      },
      "source": [
        "# Radam + lookahead"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uSATqgN9BNKk"
      },
      "source": [
        "import math\r\n",
        "import torch\r\n",
        "from torch.optim.optimizer import Optimizer, required\r\n",
        "from collections import defaultdict\r\n",
        "from itertools import chain\r\n",
        "import warnings\r\n",
        "\r\n",
        "class Lookahead(Optimizer):\r\n",
        "    def __init__(self, optimizer, k=5, alpha=0.5):\r\n",
        "        self.optimizer = optimizer\r\n",
        "        self.k = k\r\n",
        "        self.alpha = alpha\r\n",
        "        self.param_groups = self.optimizer.param_groups\r\n",
        "        self.state = defaultdict(dict)\r\n",
        "        self.fast_state = self.optimizer.state\r\n",
        "        for group in self.param_groups:\r\n",
        "            group[\"counter\"] = 0\r\n",
        "    \r\n",
        "    def update(self, group):\r\n",
        "        for fast in group[\"params\"]:\r\n",
        "            param_state = self.state[fast]\r\n",
        "            if \"slow_param\" not in param_state:\r\n",
        "                param_state[\"slow_param\"] = torch.zeros_like(fast.data)\r\n",
        "                param_state[\"slow_param\"].copy_(fast.data)\r\n",
        "            slow = param_state[\"slow_param\"]\r\n",
        "            slow += (fast.data - slow) * self.alpha\r\n",
        "            fast.data.copy_(slow)\r\n",
        "    \r\n",
        "    def update_lookahead(self):\r\n",
        "        for group in self.param_groups:\r\n",
        "            self.update(group)\r\n",
        "\r\n",
        "    def step(self, closure=None):\r\n",
        "        loss = self.optimizer.step(closure)\r\n",
        "        for group in self.param_groups:\r\n",
        "            if group[\"counter\"] == 0:\r\n",
        "                self.update(group)\r\n",
        "            group[\"counter\"] += 1\r\n",
        "            if group[\"counter\"] >= self.k:\r\n",
        "                group[\"counter\"] = 0\r\n",
        "        return loss\r\n",
        "\r\n",
        "    def state_dict(self):\r\n",
        "        fast_state_dict = self.optimizer.state_dict()\r\n",
        "        slow_state = {\r\n",
        "            (id(k) if isinstance(k, torch.Tensor) else k): v\r\n",
        "            for k, v in self.state.items()\r\n",
        "        }\r\n",
        "        fast_state = fast_state_dict[\"state\"]\r\n",
        "        param_groups = fast_state_dict[\"param_groups\"]\r\n",
        "        return {\r\n",
        "            \"fast_state\": fast_state,\r\n",
        "            \"slow_state\": slow_state,\r\n",
        "            \"param_groups\": param_groups,\r\n",
        "        }\r\n",
        "\r\n",
        "    def load_state_dict(self, state_dict):\r\n",
        "        slow_state_dict = {\r\n",
        "            \"state\": state_dict[\"slow_state\"],\r\n",
        "            \"param_groups\": state_dict[\"param_groups\"],\r\n",
        "        }\r\n",
        "        fast_state_dict = {\r\n",
        "            \"state\": state_dict[\"fast_state\"],\r\n",
        "            \"param_groups\": state_dict[\"param_groups\"],\r\n",
        "        }\r\n",
        "        super(Lookahead, self).load_state_dict(slow_state_dict)\r\n",
        "        self.optimizer.load_state_dict(fast_state_dict)\r\n",
        "        self.fast_state = self.optimizer.state\r\n",
        "\r\n",
        "    def add_param_group(self, param_group):\r\n",
        "        param_group[\"counter\"] = 0\r\n",
        "        self.optimizer.add_param_group(param_group)\r\n",
        "\r\n",
        "\r\n",
        "class RAdam(Optimizer):\r\n",
        "\r\n",
        "    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8, weight_decay=0, degenerated_to_sgd=True):\r\n",
        "        if not 0.0 <= lr:\r\n",
        "            raise ValueError(\"Invalid learning rate: {}\".format(lr))\r\n",
        "        if not 0.0 <= eps:\r\n",
        "            raise ValueError(\"Invalid epsilon value: {}\".format(eps))\r\n",
        "        if not 0.0 <= betas[0] < 1.0:\r\n",
        "            raise ValueError(\"Invalid beta parameter at index 0: {}\".format(betas[0]))\r\n",
        "        if not 0.0 <= betas[1] < 1.0:\r\n",
        "            raise ValueError(\"Invalid beta parameter at index 1: {}\".format(betas[1]))\r\n",
        "        \r\n",
        "        self.degenerated_to_sgd = degenerated_to_sgd\r\n",
        "        if isinstance(params, (list, tuple)) and len(params) > 0 and isinstance(params[0], dict):\r\n",
        "            for param in params:\r\n",
        "                if 'betas' in param and (param['betas'][0] != betas[0] or param['betas'][1] != betas[1]):\r\n",
        "                    param['buffer'] = [[None, None, None] for _ in range(10)]\r\n",
        "        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay, buffer=[[None, None, None] for _ in range(10)])\r\n",
        "        super(RAdam, self).__init__(params, defaults)\r\n",
        "\r\n",
        "    def __setstate__(self, state):\r\n",
        "        super(RAdam, self).__setstate__(state)\r\n",
        "\r\n",
        "    def step(self, closure=None):\r\n",
        "\r\n",
        "        loss = None\r\n",
        "        if closure is not None:\r\n",
        "            loss = closure()\r\n",
        "\r\n",
        "        for group in self.param_groups:\r\n",
        "\r\n",
        "            for p in group['params']:\r\n",
        "                if p.grad is None:\r\n",
        "                    continue\r\n",
        "                grad = p.grad.data.float()\r\n",
        "                if grad.is_sparse:\r\n",
        "                    raise RuntimeError('RAdam does not support sparse gradients')\r\n",
        "\r\n",
        "                p_data_fp32 = p.data.float()\r\n",
        "\r\n",
        "                state = self.state[p]\r\n",
        "\r\n",
        "                if len(state) == 0:\r\n",
        "                    state['step'] = 0\r\n",
        "                    state['exp_avg'] = torch.zeros_like(p_data_fp32)\r\n",
        "                    state['exp_avg_sq'] = torch.zeros_like(p_data_fp32)\r\n",
        "                else:\r\n",
        "                    state['exp_avg'] = state['exp_avg'].type_as(p_data_fp32)\r\n",
        "                    state['exp_avg_sq'] = state['exp_avg_sq'].type_as(p_data_fp32)\r\n",
        "\r\n",
        "                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\r\n",
        "                beta1, beta2 = group['betas']\r\n",
        "\r\n",
        "                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\r\n",
        "                exp_avg.mul_(beta1).add_(1 - beta1, grad)\r\n",
        "\r\n",
        "                state['step'] += 1\r\n",
        "                buffered = group['buffer'][int(state['step'] % 10)]\r\n",
        "                if state['step'] == buffered[0]:\r\n",
        "                    N_sma, step_size = buffered[1], buffered[2]\r\n",
        "                else:\r\n",
        "                    buffered[0] = state['step']\r\n",
        "                    beta2_t = beta2 ** state['step']\r\n",
        "                    N_sma_max = 2 / (1 - beta2) - 1\r\n",
        "                    N_sma = N_sma_max - 2 * state['step'] * beta2_t / (1 - beta2_t)\r\n",
        "                    buffered[1] = N_sma\r\n",
        "\r\n",
        "                    # more conservative since it's an approximated value\r\n",
        "                    if N_sma >= 5:\r\n",
        "                        step_size = math.sqrt((1 - beta2_t) * (N_sma - 4) / (N_sma_max - 4) * (N_sma - 2) / N_sma * N_sma_max / (N_sma_max - 2)) / (1 - beta1 ** state['step'])\r\n",
        "                    elif self.degenerated_to_sgd:\r\n",
        "                        step_size = 1.0 / (1 - beta1 ** state['step'])\r\n",
        "                    else:\r\n",
        "                        step_size = -1\r\n",
        "                    buffered[2] = step_size\r\n",
        "\r\n",
        "                # more conservative since it's an approximated value\r\n",
        "                if N_sma >= 5:\r\n",
        "                    if group['weight_decay'] != 0:\r\n",
        "                        p_data_fp32.add_(-group['weight_decay'] * group['lr'], p_data_fp32)\r\n",
        "                    denom = exp_avg_sq.sqrt().add_(group['eps'])\r\n",
        "                    p_data_fp32.addcdiv_(-step_size * group['lr'], exp_avg, denom)\r\n",
        "                    p.data.copy_(p_data_fp32)\r\n",
        "                elif step_size > 0:\r\n",
        "                    if group['weight_decay'] != 0:\r\n",
        "                        p_data_fp32.add_(-group['weight_decay'] * group['lr'], p_data_fp32)\r\n",
        "                    p_data_fp32.add_(-step_size * group['lr'], exp_avg)\r\n",
        "                    p.data.copy_(p_data_fp32)\r\n",
        "\r\n",
        "        return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gL3XsxpzaX2x"
      },
      "source": [
        "# train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OrLGgVAi--8u"
      },
      "source": [
        "\n",
        "import time\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class TorchDataset(Dataset):\n",
        "    \"\"\"\n",
        "  Format for numpy array\n",
        "  Parameters\n",
        "  ----------\n",
        "  X : 2D array\n",
        "      The input matrix\n",
        "  y : 1D array\n",
        "      Target\n",
        "  \"\"\"\n",
        "\n",
        "    def __init__(self, x, y):\n",
        "        self.x = np.expand_dims(x, 1)\n",
        "        self.y = y\n",
        "        # self.x = x\n",
        "        # self.y = y\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.x)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        x, y = self.x[index], self.y[index]\n",
        "        return x, y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OKDNOwSydzlM"
      },
      "source": [
        "import torch_xla.distributed.parallel_loader as pl\r\n",
        "import traceback\r\n",
        "\r\n",
        "def map_fn(index, params):\r\n",
        "    try:\r\n",
        "        device = xm.xla_device()\r\n",
        "        batch_size = params[\"batch_size\"]\r\n",
        "        input_dim = params[\"input_dim\"]\r\n",
        "        output_dim = params[\"output_dim\"]\r\n",
        "        hidden_dim = params[\"hidden_dim\"]\r\n",
        "        n_layers = params[\"n_layers\"]\r\n",
        "        attn_heads = params[\"attn_heads\"]\r\n",
        "        num_epochs = params[\"num_epochs\"]\r\n",
        "        num_workers = params['num_workers']\r\n",
        "    \r\n",
        "        train_sampler = torch.utils.data.distributed.DistributedSampler(\r\n",
        "            TorchDataset(X.astype(np.float32), y),\r\n",
        "            num_replicas=xm.xrt_world_size(),\r\n",
        "            rank=xm.get_ordinal(),\r\n",
        "            shuffle=True)\r\n",
        "    \r\n",
        "        test_sampler = torch.utils.data.distributed.DistributedSampler(\r\n",
        "            TorchDataset(X_test.astype(np.float32), y_test),\r\n",
        "            num_replicas=xm.xrt_world_size(),\r\n",
        "            rank=xm.get_ordinal(),\r\n",
        "            shuffle=False)\r\n",
        "    \r\n",
        "        train_loader = torch.utils.data.DataLoader(\r\n",
        "            TorchDataset(X.astype(np.float32), y),\r\n",
        "            batch_size=batch_size,\r\n",
        "            sampler=train_sampler,\r\n",
        "            num_workers=num_workers,\r\n",
        "            drop_last=True)\r\n",
        "    \r\n",
        "        test_loader = torch.utils.data.DataLoader(\r\n",
        "            TorchDataset(X_test.astype(np.float32), y_test),\r\n",
        "            batch_size=batch_size,\r\n",
        "            sampler=test_sampler,\r\n",
        "            num_workers=num_workers,\r\n",
        "            drop_last=True)\r\n",
        "    \r\n",
        "        model = Encoder(input_dim, output_dim, hidden_dim, n_layers, attn_heads).to(device)\r\n",
        "    \r\n",
        "        # Loss and optimizer\r\n",
        "        loss_fn = nn.CrossEntropyLoss()\r\n",
        "        optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, betas=(0.9, 0.999))  # Any optimizer\r\n",
        "        lookahead = Lookahead(optimizer, k=5, alpha=0.5)  # Initialize Lookahead\r\n",
        "        total_step = len(train_loader)\r\n",
        "    \r\n",
        "        train_start = time.time()\r\n",
        "        for epoch in range(num_epochs):\r\n",
        "            para_train_loader = pl.ParallelLoader(train_loader, [device]).per_device_loader(device)\r\n",
        "            for i, (inputs, labels) in enumerate(para_train_loader):\r\n",
        "                # Move tensors to the configured device\r\n",
        "                inputs = inputs.to(device)\r\n",
        "                labels = labels.to(device)\r\n",
        "    \r\n",
        "                # Forward pass\r\n",
        "                outputs = model(inputs)\r\n",
        "                outputs = outputs.squeeze(1)\r\n",
        "                loss = loss_fn(outputs, labels)\r\n",
        "    \r\n",
        "                # Backward and optimize\r\n",
        "                lookahead.zero_grad()\r\n",
        "                loss.backward(retain_graph=True)  # Self-defined loss function\r\n",
        "                lookahead.step()\r\n",
        "    \r\n",
        "                '''\r\n",
        "                optimizer.zero_grad()\r\n",
        "                loss.backward(retain_graph=True) \r\n",
        "                optimizer.step()\r\n",
        "                '''\r\n",
        "                # if (i + 1) % 5 == 0:\r\n",
        "                #     print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'\r\n",
        "                #           .format(epoch + 1, num_epochs, i + 1, total_step, loss.item()))\r\n",
        "                #     with torch.no_grad():\r\n",
        "                #         correct = 0\r\n",
        "                #         total = 0\r\n",
        "                #         for inputs, labels in test_loader:\r\n",
        "                #             inputs = inputs.to(device)\r\n",
        "                #             labels = labels.to(device)\r\n",
        "                #             outputs = model(inputs)\r\n",
        "                #             outputs = outputs.squeeze(1)\r\n",
        "                #             _, predicted = torch.max(outputs.data, 1)\r\n",
        "                #             total += labels.size(0)\r\n",
        "                #             correct += (predicted == labels).sum().item()\r\n",
        "    \r\n",
        "                        # print('Accuracy: {} %'.format(100 * correct / total))\r\n",
        "        elapsed_train_time = time.time() - train_start\r\n",
        "        print(\"Process\", index, \"finished training. Train time was:\", elapsed_train_time)\r\n",
        "\r\n",
        "    except:\r\n",
        "        traceback.print_exc()\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 858
        },
        "id": "PfC3auF7H5il",
        "outputId": "e8085c1e-9790-490a-eae1-9bf0e84b6d53"
      },
      "source": [
        "\n",
        "import torch_xla.distributed.xla_multiprocessing as xmp\n",
        "\n",
        "params = {}\n",
        "params['batch_size'] = 32\n",
        "params['num_workers'] = 8\n",
        "params['input_dim'] = 93\n",
        "params['hidden_dim'] = 256\n",
        "params['output_dim'] = 2\n",
        "params['attn_heads'] = 1\n",
        "params['dim_feedforward'] = 32\n",
        "params['n_layers'] = 12\n",
        "params['learning_rate'] = 1e-3\n",
        "params['num_epochs'] = 2\n",
        "\n",
        "xmp.spawn(map_fn, args=(params,), nprocs=8, start_method='fork')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:29: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:29: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:29: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:29: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "Exception in device=TPU:2: tensorflow/compiler/xla/xla_client/mesh_service.cc:331 : Failed to retrieve mesh configuration: Connection reset by peer (14)\n",
            "Exception in device=TPU:4: tensorflow/compiler/xla/xla_client/mesh_service.cc:331 : Failed to retrieve mesh configuration: Connection reset by peer (14)\n",
            "Exception in device=TPU:3: tensorflow/compiler/xla/xla_client/mesh_service.cc:331 : Failed to retrieve mesh configuration: Connection reset by peer (14)\n",
            "Traceback (most recent call last):\n",
            "Traceback (most recent call last):\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch_xla/distributed/xla_multiprocessing.py\", line 329, in _mp_start_fn\n",
            "    _start_fn(index, pf_cfg, fn, args)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch_xla/distributed/xla_multiprocessing.py\", line 329, in _mp_start_fn\n",
            "    _start_fn(index, pf_cfg, fn, args)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch_xla/distributed/xla_multiprocessing.py\", line 322, in _start_fn\n",
            "    _setup_replication()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch_xla/distributed/xla_multiprocessing.py\", line 314, in _setup_replication\n",
            "    device = xm.xla_device()\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "ProcessExitedException",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mProcessExitedException\u001b[0m                    Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-5ad4d1dfb009>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'num_epochs'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mxmp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspawn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnprocs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_method\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'fork'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch_xla/distributed/xla_multiprocessing.py\u001b[0m in \u001b[0;36mspawn\u001b[0;34m(fn, args, nprocs, join, daemon, start_method)\u001b[0m\n\u001b[1;32m    392\u001b[0m         \u001b[0mjoin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m         \u001b[0mdaemon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdaemon\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 394\u001b[0;31m         start_method=start_method)\n\u001b[0m\u001b[1;32m    395\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/multiprocessing/spawn.py\u001b[0m in \u001b[0;36mstart_processes\u001b[0;34m(fn, args, nprocs, join, daemon, start_method)\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;31m# Loop on join until it returns True or raises an exception.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m         \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/multiprocessing/spawn.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    134\u001b[0m                     \u001b[0merror_pid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfailed_process\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpid\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m                     \u001b[0mexit_code\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexitcode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m                     \u001b[0msignal_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m                 )\n\u001b[1;32m    138\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mProcessExitedException\u001b[0m: process 0 terminated with signal SIGSEGV"
          ]
        }
      ]
    }
  ]
}
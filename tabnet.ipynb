{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tabnet.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sugangnb/ai-research/blob/main/tabnet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1jxRBdC4A719"
      },
      "source": [
        "import os\n",
        "assert os.environ['COLAB_TPU_ADDR'], 'Make sure to select TPU from Edit > Notebook settings > Hardware accelerator'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PsKsFirmVPVo",
        "outputId": "69885f62-087a-4295-d442-9dc3c5f774fd"
      },
      "source": [
        "!rm -rf kicked_dataset/\r\n",
        "!git clone https://github.com/calibertytz/kicked_dataset.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'kicked_dataset'...\n",
            "remote: Enumerating objects: 3, done.\u001b[K\n",
            "remote: Counting objects: 100% (3/3), done.\u001b[K\n",
            "remote: Compressing objects: 100% (3/3), done.\u001b[K\n",
            "remote: Total 25 (delta 0), reused 0 (delta 0), pack-reused 22\u001b[K\n",
            "Unpacking objects: 100% (25/25), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RCxJ4WBlASfq",
        "outputId": "9d195d3c-39cc-4dc4-d17d-a524e34dd89b"
      },
      "source": [
        "VERSION = \"nightly\"  #@param [\"1.5\" , \"20200325\", \"nightly\"]\n",
        "!curl https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py\n",
        "!python pytorch-xla-env-setup.py --version $VERSION"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  5116  100  5116    0     0  23906      0 --:--:-- --:--:-- --:--:-- 23906\n",
            "Updating... This may take around 2 minutes.\n",
            "Updating TPU runtime to pytorch-nightly ...\n",
            "Uninstalling torch-1.8.0a0:\n",
            "  Successfully uninstalled torch-1.8.0a0\n",
            "Uninstalling torchvision-0.9.0a0+d0063f3:\n",
            "  Successfully uninstalled torchvision-0.9.0a0+d0063f3\n",
            "Copying gs://tpu-pytorch/wheels/torch-nightly-cp36-cp36m-linux_x86_64.whl...\n",
            "\n",
            "Operation completed over 1 objects/122.1 MiB.                                    \n",
            "Copying gs://tpu-pytorch/wheels/torch_xla-nightly-cp36-cp36m-linux_x86_64.whl...\n",
            "- [1 files][131.0 MiB/131.0 MiB]                                                \n",
            "Operation completed over 1 objects/131.0 MiB.                                    \n",
            "Copying gs://tpu-pytorch/wheels/torchvision-nightly-cp36-cp36m-linux_x86_64.whl...\n",
            "/ [1 files][  4.8 MiB/  4.8 MiB]                                                \n",
            "Operation completed over 1 objects/4.8 MiB.                                      \n",
            "Processing ./torch-nightly-cp36-cp36m-linux_x86_64.whl\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch==nightly) (3.7.4.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch==nightly) (1.19.5)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from torch==nightly) (0.8)\n",
            "\u001b[31mERROR: fastai 1.0.61 requires torchvision, which is not installed.\u001b[0m\n",
            "Installing collected packages: torch\n",
            "Successfully installed torch-1.8.0a0\n",
            "Done updating TPU runtime\n",
            "Processing ./torch_xla-nightly-cp36-cp36m-linux_x86_64.whl\n",
            "Installing collected packages: torch-xla\n",
            "  Found existing installation: torch-xla 1.6+ab2ceda\n",
            "    Uninstalling torch-xla-1.6+ab2ceda:\n",
            "      Successfully uninstalled torch-xla-1.6+ab2ceda\n",
            "Successfully installed torch-xla-1.6+ab2ceda\n",
            "Processing ./torchvision-nightly-cp36-cp36m-linux_x86_64.whl\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision==nightly) (1.19.5)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from torchvision==nightly) (1.8.0a0)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision==nightly) (7.0.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch->torchvision==nightly) (3.7.4.3)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from torch->torchvision==nightly) (0.8)\n",
            "Installing collected packages: torchvision\n",
            "Successfully installed torchvision-0.9.0a0+d0063f3\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "libomp5 is already the newest version (5.0.1-1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 16 not upgraded.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oEj05oHzVgSq"
      },
      "source": [
        "import pandas as pd\r\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lp-ntRjRWII0"
      },
      "source": [
        "df_train = pd.read_csv('kicked_dataset/df_train_final.csv')\r\n",
        "df_test = pd.read_csv('kicked_dataset/df_test_final.csv')\r\n",
        "\r\n",
        "X = df_train.drop(columns=['label']).values\r\n",
        "y = df_train['label'].values\r\n",
        "\r\n",
        "X_test = df_test.drop(columns=['label']).values\r\n",
        "y_test = df_test['label'].values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bLM-BLDdZrQB"
      },
      "source": [
        "# Attention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ihum7X038fM-"
      },
      "source": [
        "from torch import nn\r\n",
        "from torch.autograd import Function\r\n",
        "import torch.nn.functional as F\r\n",
        "import torch\r\n",
        "import torch_xla.core.xla_model as xm\r\n",
        "\r\n",
        "class EntmaxBisectFunction(Function):\r\n",
        "    @classmethod\r\n",
        "    def _gp(cls, x, alpha):\r\n",
        "        return x ** (alpha - 1)\r\n",
        "\r\n",
        "    @classmethod\r\n",
        "    def _gp_inv(cls, y, alpha):\r\n",
        "\r\n",
        "        return y ** (1 / (alpha - 1))\r\n",
        "\r\n",
        "    @classmethod\r\n",
        "    def _p(cls, X, alpha):\r\n",
        "        return cls._gp_inv(torch.clamp(X, min=0), alpha)\r\n",
        "\r\n",
        "    @classmethod\r\n",
        "    def forward(cls, ctx, X, alpha=1.5, dim=-1, n_iter=50, ensure_sum_one=True):\r\n",
        "        \r\n",
        "        device = X.device\r\n",
        "        if not isinstance(alpha, torch.Tensor):\r\n",
        "            alpha = torch.tensor(alpha, dtype=X.dtype, device=device)\r\n",
        "        if not xm.is_xla_tensor(alpha):\r\n",
        "            # alpha = alpha.clone().detach().requires_grad_(True)\r\n",
        "            alpha = torch.tensor(alpha, dtype=X.dtype, device=device)\r\n",
        "\r\n",
        "        alpha_shape = list(X.shape)\r\n",
        "        alpha_shape[dim] = 1\r\n",
        "        alpha = alpha.expand(*alpha_shape)\r\n",
        "        ctx.alpha = alpha\r\n",
        "        ctx.dim = dim\r\n",
        "        d = X.shape[dim]\r\n",
        "        X = X * (alpha - 1)\r\n",
        "\r\n",
        "        max_val, _ = X.max(dim=dim, keepdim=True)\r\n",
        "\r\n",
        "        tau_lo = max_val - cls._gp(1, alpha)\r\n",
        "        tau_hi = max_val - cls._gp(1 / d, alpha)\r\n",
        "        f_lo = cls._p(X - tau_lo, alpha).sum(dim) - 1\r\n",
        "        dm = tau_hi - tau_lo\r\n",
        "        for it in range(n_iter):\r\n",
        "\r\n",
        "            dm /= 2\r\n",
        "            tau_m = tau_lo + dm\r\n",
        "            p_m = cls._p(X - tau_m, alpha)\r\n",
        "            f_m = p_m.sum(dim) - 1\r\n",
        "\r\n",
        "            mask = (f_m * f_lo >= 0).unsqueeze(dim)\r\n",
        "            tau_lo = torch.where(mask, tau_m, tau_lo)\r\n",
        "\r\n",
        "        if ensure_sum_one:\r\n",
        "            p_m /= p_m.sum(dim=dim).unsqueeze(dim=dim)\r\n",
        "        \r\n",
        "        ctx.save_for_backward(xm.all_gather(p_m,dim=dim))\r\n",
        "        return xm.all_gather(p_m,dim=dim)\r\n",
        "        # ctx.save_for_backward(p_m)\r\n",
        "        # return p_m\r\n",
        "        \r\n",
        "    @classmethod\r\n",
        "    def backward(cls, ctx, dY):\r\n",
        "        Y, = ctx.saved_tensors\r\n",
        "\r\n",
        "        gppr = torch.where(Y > 0, Y ** (2 - ctx.alpha), Y.new_zeros(1))\r\n",
        "\r\n",
        "        dX = dY * gppr\r\n",
        "        q = dX.sum(ctx.dim) / gppr.sum(ctx.dim)\r\n",
        "        q = q.unsqueeze(ctx.dim)\r\n",
        "        dX -= q * gppr\r\n",
        "\r\n",
        "        d_alpha = None\r\n",
        "        if ctx.needs_input_grad[1]:\r\n",
        "\r\n",
        "            # alpha gradient computation\r\n",
        "            # d_alpha = (partial_y / partial_alpha) * dY\r\n",
        "            # NOTE: ensure alpha is not close to 1\r\n",
        "            # since there is an indetermination\r\n",
        "            # batch_size, _ = dY.shape\r\n",
        "\r\n",
        "            # shannon terms\r\n",
        "            S = torch.where(Y > 0, - Y * torch.log(Y), Y.new_zeros(1))\r\n",
        "\r\n",
        "            # shannon entropy\r\n",
        "            ent = S.sum(ctx.dim).unsqueeze(ctx.dim)\r\n",
        "            Y_skewed = gppr / gppr.sum(ctx.dim).unsqueeze(ctx.dim)\r\n",
        "\r\n",
        "            d_alpha = dY * (Y - Y_skewed) / ((ctx.alpha - 1) ** 2)\r\n",
        "            d_alpha += dY * (S - Y_skewed * ent) / (ctx.alpha - 1)\r\n",
        "            d_alpha = d_alpha.sum(ctx.dim).unsqueeze(ctx.dim)\r\n",
        "\r\n",
        "        return dX, d_alpha, None, None, None\r\n",
        "\r\n",
        "\r\n",
        "# slightly more efficient special case for sparsemax\r\n",
        "class SparsemaxBisectFunction(EntmaxBisectFunction):\r\n",
        "    @classmethod\r\n",
        "    def _gp(cls, x, alpha):\r\n",
        "        return x\r\n",
        "\r\n",
        "    @classmethod\r\n",
        "    def _gp_inv(cls, y, alpha):\r\n",
        "        return y\r\n",
        "\r\n",
        "    @classmethod\r\n",
        "    def _p(cls, x, alpha):\r\n",
        "        return torch.clamp(x, min=0)\r\n",
        "\r\n",
        "    @classmethod\r\n",
        "    def forward(cls, ctx, X, dim=-1, n_iter=50, ensure_sum_one=True):\r\n",
        "        return super().forward(\r\n",
        "            ctx, X, alpha=2, dim=dim, n_iter=50, ensure_sum_one=True\r\n",
        "        )\r\n",
        "\r\n",
        "    @classmethod\r\n",
        "    def backward(cls, ctx, dY):\r\n",
        "        Y, = ctx.saved_tensors\r\n",
        "        gppr = (Y > 0).to(dtype=dY.dtype)\r\n",
        "        dX = dY * gppr\r\n",
        "        q = dX.sum(ctx.dim) / gppr.sum(ctx.dim)\r\n",
        "        q = q.unsqueeze(ctx.dim)\r\n",
        "        dX -= q * gppr\r\n",
        "        return dX, None, None, None\r\n",
        "\r\n",
        "\r\n",
        "def entmax_bisect(X, alpha=1.5, dim=-1, n_iter=50, ensure_sum_one=True):\r\n",
        "    \"\"\"alpha-entmax: normalizing sparse transform (a la softmax).\r\n",
        "    Solves the optimization problem:\r\n",
        "        max_p <x, p> - H_a(p)    s.t.    p >= 0, sum(p) == 1.\r\n",
        "    where H_a(p) is the Tsallis alpha-entropy with custom alpha >= 1,\r\n",
        "    using a bisection (root finding, binary search) algorithm.\r\n",
        "    This function is differentiable with respect to both X and alpha.\r\n",
        "    Parameters\r\n",
        "    ----------\r\n",
        "    X : torch.Tensor\r\n",
        "        The input tensor.\r\n",
        "    alpha : float or torch.Tensor\r\n",
        "        Tensor of alpha parameters (> 1) to use. If scalar\r\n",
        "        or python float, the same value is used for all rows, otherwise,\r\n",
        "        it must have shape (or be expandable to)\r\n",
        "        alpha.shape[j] == (X.shape[j] if j != dim else 1)\r\n",
        "        A value of alpha=2 corresponds to sparsemax, and alpha=1 corresponds to\r\n",
        "        softmax (but computing it this way is likely unstable).\r\n",
        "    dim : int\r\n",
        "        The dimension along which to apply alpha-entmax.\r\n",
        "    n_iter : int\r\n",
        "        Number of bisection iterations. For float32, 24 iterations should\r\n",
        "        suffice for machine precision.\r\n",
        "    ensure_sum_one : bool,\r\n",
        "        Whether to divide the result by its sum. If false, the result might\r\n",
        "        sum to close but not exactly 1, which might cause downstream problems.\r\n",
        "    Returns\r\n",
        "    -------\r\n",
        "    P : torch tensor, same shape as X\r\n",
        "        The projection result, such that P.sum(dim=dim) == 1 elementwise.\r\n",
        "    \"\"\"\r\n",
        "    return EntmaxBisectFunction.apply(X, alpha, dim, n_iter, ensure_sum_one)\r\n",
        "\r\n",
        "\r\n",
        "def sparsemax_bisect(X, dim=-1, n_iter=50, ensure_sum_one=True):\r\n",
        "    \"\"\"sparsemax: normalizing sparse transform (a la softmax), via bisection.\r\n",
        "    Solves the projection:\r\n",
        "        min_p ||x - p||_2   s.t.    p >= 0, sum(p) == 1.\r\n",
        "    Parameters\r\n",
        "    ----------\r\n",
        "    X : torch.Tensor\r\n",
        "        The input tensor.\r\n",
        "    dim : int\r\n",
        "        The dimension along which to apply sparsemax.\r\n",
        "    n_iter : int\r\n",
        "        Number of bisection iterations. For float32, 24 iterations should\r\n",
        "        suffice for machine precision.\r\n",
        "    ensure_sum_one : bool,\r\n",
        "        Whether to divide the result by its sum. If false, the result might\r\n",
        "        sum to close but not exactly 1, which might cause downstream problems.\r\n",
        "    Note: This function does not yet support normalizing along anything except\r\n",
        "    the last dimension. Please use transposing and views to achieve more\r\n",
        "    general behavior.\r\n",
        "    Returns\r\n",
        "    -------\r\n",
        "    P : torch tensor, same shape as X\r\n",
        "        The projection result, such that P.sum(dim=dim) == 1 elementwise.\r\n",
        "    \"\"\"\r\n",
        "    return SparsemaxBisectFunction.apply(X, dim, n_iter, ensure_sum_one)\r\n",
        "\r\n",
        "\r\n",
        "class SparsemaxBisect(nn.Module):\r\n",
        "    def __init__(self, dim=-1, n_iter=None):\r\n",
        "        \"\"\"sparsemax: normalizing sparse transform (a la softmax) via bisection\r\n",
        "        Solves the projection:\r\n",
        "            min_p ||x - p||_2   s.t.    p >= 0, sum(p) == 1.\r\n",
        "        Parameters\r\n",
        "        ----------\r\n",
        "        dim : int\r\n",
        "            The dimension along which to apply sparsemax.\r\n",
        "        n_iter : int\r\n",
        "            Number of bisection iterations. For float32, 24 iterations should\r\n",
        "            suffice for machine precision.\r\n",
        "        \"\"\"\r\n",
        "        self.dim = dim\r\n",
        "        self.n_iter = n_iter\r\n",
        "        super().__init__()\r\n",
        "\r\n",
        "    def forward(self, X):\r\n",
        "        return sparsemax_bisect(X, dim=self.dim, n_iter=self.n_iter)\r\n",
        "\r\n",
        "\r\n",
        "class EntmaxBisect(nn.Module):\r\n",
        "    def __init__(self, alpha=1.8, dim=-1, n_iter=50):\r\n",
        "        \"\"\"alpha-entmax: normalizing sparse map (a la softmax) via bisection.\r\n",
        "        Solves the optimization problem:\r\n",
        "            max_p <x, p> - H_a(p)    s.t.    p >= 0, sum(p) == 1.\r\n",
        "        where H_a(p) is the Tsallis alpha-entropy with custom alpha >= 1,\r\n",
        "        using a bisection (root finding, binary search) algorithm.\r\n",
        "        Parameters\r\n",
        "        ----------\r\n",
        "        alpha : float or torch.Tensor\r\n",
        "            Tensor of alpha parameters (> 1) to use. If scalar\r\n",
        "            or python float, the same value is used for all rows, otherwise,\r\n",
        "            it must have shape (or be expandable to)\r\n",
        "            alpha.shape[j] == (X.shape[j] if j != dim else 1)\r\n",
        "            A value of alpha=2 corresponds to sparsemax; alpha=1 corresponds\r\n",
        "            to softmax (but computing it this way is likely unstable).\r\n",
        "        dim : int\r\n",
        "            The dimension along which to apply alpha-entmax.\r\n",
        "        n_iter : int\r\n",
        "            Number of bisection iterations. For float32, 24 iterations should\r\n",
        "            suffice for machine precision.\r\n",
        "        \"\"\"\r\n",
        "        self.dim = dim\r\n",
        "        self.n_iter = n_iter\r\n",
        "        self.alpha = alpha\r\n",
        "        super().__init__()\r\n",
        "\r\n",
        "    def forward(self, X):\r\n",
        "        return entmax_bisect(\r\n",
        "            X, alpha=self.alpha, dim=self.dim, n_iter=self.n_iter\r\n",
        "        )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2hoi1JagZxsH"
      },
      "source": [
        "import math\r\n",
        "\r\n",
        "\r\n",
        "class Attention(nn.Module):\r\n",
        "    \"\"\"\r\n",
        "    Compute 'Scaled Dot Product Attention\r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    def __init__(self, alpha=1.5):\r\n",
        "        super(Attention, self).__init__()\r\n",
        "        self.alpha = alpha * nn.Parameter(torch.ones([1]))\r\n",
        "        self.activation = EntmaxBisect(alpha=self.alpha, dim=-1)\r\n",
        "\r\n",
        "    def forward(self, query, key, value, mask=None, dropout=None):\r\n",
        "        scores = torch.matmul(query, key.transpose(-2, -1)) \\\r\n",
        "                 / math.sqrt(query.size(-1))\r\n",
        "\r\n",
        "        if mask is not None:\r\n",
        "            scores = scores.masked_fill(mask == 0, -1e9)\r\n",
        "\r\n",
        "        p_attn = self.activation(scores)\r\n",
        "\r\n",
        "        if dropout is not None:\r\n",
        "            p_attn = dropout(p_attn)\r\n",
        "        \r\n",
        "        return torch.matmul(p_attn, value), p_attn\r\n",
        "\r\n",
        "class MultiHeadedAttention(nn.Module):\r\n",
        "    \"\"\"\r\n",
        "    Take in model size and number of heads.\r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    def __init__(self, h, d_model, dropout=0.1):\r\n",
        "        super().__init__()\r\n",
        "        assert d_model % h == 0\r\n",
        "\r\n",
        "        # We assume d_v always equals d_k\r\n",
        "        self.d_k = d_model // h\r\n",
        "        self.h = h\r\n",
        "\r\n",
        "        self.linear_layers = nn.ModuleList([nn.Linear(d_model, d_model) for _ in range(3)])\r\n",
        "        self.output_linear = nn.Linear(d_model, d_model)\r\n",
        "        self.attention = Attention()\r\n",
        "\r\n",
        "        self.dropout = nn.Dropout(p=dropout)\r\n",
        "\r\n",
        "    def forward(self, query, key, value, mask=None):\r\n",
        "        batch_size = query.size(0)\r\n",
        "\r\n",
        "        # 1) Do all the linear projections in batch from d_model => h x d_k\r\n",
        "        query, key, value = [l(x).view(batch_size, -1, self.h, self.d_k).transpose(1, 2)\r\n",
        "                             for l, x in zip(self.linear_layers, (query, key, value))]\r\n",
        "\r\n",
        "        # 2) Apply attention on all the projected vectors in batch.\r\n",
        "        x, attn = self.attention(query, key, value, mask=mask, dropout=self.dropout)\r\n",
        "\r\n",
        "        # 3) \"Concat\" using a view and apply a final linear.\r\n",
        "        x = x.transpose(1, 2).contiguous().view(batch_size, -1, self.h * self.d_k)\r\n",
        "\r\n",
        "        return self.output_linear(x)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "idN91fFvZ96M"
      },
      "source": [
        "# utils"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C4gyFGumZ9Qp"
      },
      "source": [
        "import torch.nn as nn\r\n",
        "import torch\r\n",
        "\r\n",
        "class PositionwiseFeedForward(nn.Module):\r\n",
        "    \"Implements FFN equation.\"\r\n",
        "    def __init__(self, d_model, d_ff, dropout=0.1):\r\n",
        "        super(PositionwiseFeedForward, self).__init__()\r\n",
        "        self.w_1 = nn.Linear(d_model, d_ff)\r\n",
        "        self.w_2 = nn.Linear(d_ff, d_model)\r\n",
        "        self.dropout = nn.Dropout(dropout)\r\n",
        "        self.activation = nn.GELU()\r\n",
        "\r\n",
        "    def forward(self, x):\r\n",
        "        return self.w_2(self.dropout(self.activation(self.w_1(x))))\r\n",
        "\r\n",
        "\r\n",
        "class LayerNorm(nn.Module):\r\n",
        "    \"Construct a layernorm module (See citation for details).\"\r\n",
        "\r\n",
        "    def __init__(self, features, eps=1e-12):\r\n",
        "        super(LayerNorm, self).__init__()\r\n",
        "        self.a_2 = nn.Parameter(torch.ones(features))\r\n",
        "        self.b_2 = nn.Parameter(torch.zeros(features))\r\n",
        "        self.eps = eps\r\n",
        "\r\n",
        "    def forward(self, x):\r\n",
        "        mean = x.mean(-1, keepdim=True)\r\n",
        "        std = x.std(-1, keepdim=True)\r\n",
        "        return self.a_2 * (x - mean) / (std + self.eps) + self.b_2\r\n",
        "\r\n",
        "class SublayerConnection(nn.Module):\r\n",
        "    \"\"\"\r\n",
        "    A residual connection followed by a layer norm.\r\n",
        "    Note for code simplicity the norm is first as opposed to last.\r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    def __init__(self, size, dropout):\r\n",
        "        super(SublayerConnection, self).__init__()\r\n",
        "        self.norm = LayerNorm(size)\r\n",
        "        self.dropout = nn.Dropout(dropout)\r\n",
        "\r\n",
        "    def forward(self, x, sublayer):\r\n",
        "        \"Apply residual connection to any sublayer with the same size.\"\r\n",
        "        return x + self.dropout(sublayer(self.norm(x)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "txom5RSjaN1C"
      },
      "source": [
        "# transformer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4kIoEh9jaMyR"
      },
      "source": [
        "class TransformerBlock(nn.Module):\r\n",
        "    \"\"\"\r\n",
        "    Bidirectional Encoder = Transformer (self-attention)\r\n",
        "    Transformer = MultiHead_Attention + Feed_Forward with sublayer connection\r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    def __init__(self, hidden, attn_heads, feed_forward_hidden, dropout):\r\n",
        "        \"\"\"\r\n",
        "        :param hidden: hidden size of transformer\r\n",
        "        :param attn_heads: head sizes of multi-head attention\r\n",
        "        :param feed_forward_hidden: feed_forward_hidden, usually 4*hidden_size\r\n",
        "        :param dropout: dropout rate\r\n",
        "        \"\"\"\r\n",
        "\r\n",
        "        super().__init__()\r\n",
        "        self.attention = MultiHeadedAttention(h=attn_heads, d_model=hidden)\r\n",
        "        self.feed_forward = PositionwiseFeedForward(d_model=hidden, d_ff=feed_forward_hidden, dropout=dropout)\r\n",
        "        self.input_sublayer = SublayerConnection(size=hidden, dropout=dropout)\r\n",
        "        self.output_sublayer = SublayerConnection(size=hidden, dropout=dropout)\r\n",
        "        self.dropout = nn.Dropout(p=dropout)\r\n",
        "\r\n",
        "    def forward(self, x, mask):\r\n",
        "        x = self.input_sublayer(x, lambda _x: self.attention.forward(_x, _x, _x, mask=mask))\r\n",
        "        x = self.output_sublayer(x, self.feed_forward)\r\n",
        "        return self.dropout(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J1fnJFLCaSo5"
      },
      "source": [
        "# model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hMEuRynKaT51"
      },
      "source": [
        "import torch.nn.functional as F\r\n",
        "import torch_xla.core.xla_model as xm\r\n",
        "\r\n",
        "\r\n",
        "class FeedForwardBlock(nn.Module):\r\n",
        "    '''\r\n",
        "    use rezero\r\n",
        "    '''\r\n",
        "\r\n",
        "    def __init__(self, in_dim, out_dim, dim_feedforward=2048, dropout=0.1, activation='relu'):\r\n",
        "        super(FeedForwardBlock, self).__init__()\r\n",
        "        self.linear1 = nn.Linear(in_dim, dim_feedforward)\r\n",
        "        self.dropout1 = nn.Dropout(dropout)\r\n",
        "        self.dropout2 = nn.Dropout(dropout)\r\n",
        "        self.linear2 = nn.Linear(dim_feedforward, out_dim)\r\n",
        "        self.resweight = nn.Parameter(torch.Tensor([0]))\r\n",
        "\r\n",
        "        if activation == \"relu\":\r\n",
        "            self.activation = F.relu\r\n",
        "        elif activation == \"gelu\":\r\n",
        "            self.activation = F.gelu\r\n",
        "\r\n",
        "    def forward(self, src):\r\n",
        "        src2 = src\r\n",
        "        src2 = self.linear2(self.dropout1(self.activation(self.linear1(src2))))\r\n",
        "        src2 = src2 * self.resweight\r\n",
        "        src = src + self.dropout2(src2)\r\n",
        "\r\n",
        "        return src\r\n",
        "\r\n",
        "\r\n",
        "class Encoder(nn.Module):\r\n",
        "    def __init__(self,\r\n",
        "                 input_dim,\r\n",
        "                 output_dim,\r\n",
        "                 hidden_dim,\r\n",
        "                 n_layers,\r\n",
        "                 attn_heads,\r\n",
        "                 dropout=0.1):\r\n",
        "\r\n",
        "        super().__init__()\r\n",
        "        self.input_dim = input_dim\r\n",
        "        self.output_dim = output_dim\r\n",
        "        self.hidden_dim = hidden_dim\r\n",
        "        self.n_layers = n_layers\r\n",
        "        self.attn_heads = attn_heads\r\n",
        "        self.dropout = dropout\r\n",
        "\r\n",
        "        self.embedding = nn.Linear(input_dim, hidden_dim)  # TODO: add attentive embedding\r\n",
        "\r\n",
        "        self.shared_base_block = TransformerBlock(self.hidden_dim, self.attn_heads, self.hidden_dim * 4, self.dropout)\r\n",
        "\r\n",
        "        self.shared_layer_tail = nn.ModuleList([FeedForwardBlock(in_dim=self.hidden_dim, out_dim=self.hidden_dim) for _\r\n",
        "                                                in range(int(self.n_layers / 2))])\r\n",
        "\r\n",
        "        self.no_shared_layer_head = nn.ModuleList(\r\n",
        "            TransformerBlock(self.hidden_dim, self.attn_heads, self.hidden_dim * 4, self.dropout) for _\r\n",
        "            in range(int(self.n_layers / 2)))\r\n",
        "        self.no_shared_layer_tail = nn.ModuleList(\r\n",
        "            FeedForwardBlock(in_dim=self.hidden_dim, out_dim=self.hidden_dim) for _\r\n",
        "            in range(int(self.n_layers / 2)))\r\n",
        "\r\n",
        "        self.final_layer = nn.Sequential(nn.Sigmoid(),\r\n",
        "                                         nn.Linear(self.hidden_dim, self.output_dim))\r\n",
        "\r\n",
        "    def forward(self, x):\r\n",
        "        x = self.embedding(x)\r\n",
        "        for i in range(self.n_layers):\r\n",
        "            if i % 2 != 0:\r\n",
        "                x = self.shared_base_block(x, mask=None)\r\n",
        "                x = self.shared_layer_tail[int(i / 2)](x)\r\n",
        "            else:\r\n",
        "                x = self.no_shared_layer_head[int(i / 2)](x, mask=None)\r\n",
        "                x = self.no_shared_layer_tail[int(i / 2)](x)\r\n",
        "        print(\"self.final_layer(x)\")\r\n",
        "        print(self.final_layer(x))\r\n",
        "        return self.final_layer(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rSBdCMX9BN2c"
      },
      "source": [
        "# Radam + lookahead"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uSATqgN9BNKk"
      },
      "source": [
        "import math\r\n",
        "import torch\r\n",
        "from torch.optim.optimizer import Optimizer, required\r\n",
        "from collections import defaultdict\r\n",
        "from itertools import chain\r\n",
        "import warnings\r\n",
        "\r\n",
        "class Lookahead(Optimizer):\r\n",
        "    def __init__(self, optimizer, k=5, alpha=0.5):\r\n",
        "        self.optimizer = optimizer\r\n",
        "        self.k = k\r\n",
        "        self.alpha = alpha\r\n",
        "        self.param_groups = self.optimizer.param_groups\r\n",
        "        self.state = defaultdict(dict)\r\n",
        "        self.fast_state = self.optimizer.state\r\n",
        "        for group in self.param_groups:\r\n",
        "            group[\"counter\"] = 0\r\n",
        "    \r\n",
        "    def update(self, group):\r\n",
        "        for fast in group[\"params\"]:\r\n",
        "            param_state = self.state[fast]\r\n",
        "            if \"slow_param\" not in param_state:\r\n",
        "                param_state[\"slow_param\"] = torch.zeros_like(fast.data)\r\n",
        "                param_state[\"slow_param\"].copy_(fast.data)\r\n",
        "            slow = param_state[\"slow_param\"]\r\n",
        "            slow += (fast.data - slow) * self.alpha\r\n",
        "            fast.data.copy_(slow)\r\n",
        "    \r\n",
        "    def update_lookahead(self):\r\n",
        "        for group in self.param_groups:\r\n",
        "            self.update(group)\r\n",
        "\r\n",
        "    def step(self, closure=None):\r\n",
        "        loss = self.optimizer.step(closure)\r\n",
        "        for group in self.param_groups:\r\n",
        "            if group[\"counter\"] == 0:\r\n",
        "                self.update(group)\r\n",
        "            group[\"counter\"] += 1\r\n",
        "            if group[\"counter\"] >= self.k:\r\n",
        "                group[\"counter\"] = 0\r\n",
        "        return loss\r\n",
        "\r\n",
        "    def state_dict(self):\r\n",
        "        fast_state_dict = self.optimizer.state_dict()\r\n",
        "        slow_state = {\r\n",
        "            (id(k) if isinstance(k, torch.Tensor) else k): v\r\n",
        "            for k, v in self.state.items()\r\n",
        "        }\r\n",
        "        fast_state = fast_state_dict[\"state\"]\r\n",
        "        param_groups = fast_state_dict[\"param_groups\"]\r\n",
        "        return {\r\n",
        "            \"fast_state\": fast_state,\r\n",
        "            \"slow_state\": slow_state,\r\n",
        "            \"param_groups\": param_groups,\r\n",
        "        }\r\n",
        "\r\n",
        "    def load_state_dict(self, state_dict):\r\n",
        "        slow_state_dict = {\r\n",
        "            \"state\": state_dict[\"slow_state\"],\r\n",
        "            \"param_groups\": state_dict[\"param_groups\"],\r\n",
        "        }\r\n",
        "        fast_state_dict = {\r\n",
        "            \"state\": state_dict[\"fast_state\"],\r\n",
        "            \"param_groups\": state_dict[\"param_groups\"],\r\n",
        "        }\r\n",
        "        super(Lookahead, self).load_state_dict(slow_state_dict)\r\n",
        "        self.optimizer.load_state_dict(fast_state_dict)\r\n",
        "        self.fast_state = self.optimizer.state\r\n",
        "\r\n",
        "    def add_param_group(self, param_group):\r\n",
        "        param_group[\"counter\"] = 0\r\n",
        "        self.optimizer.add_param_group(param_group)\r\n",
        "\r\n",
        "\r\n",
        "class RAdam(Optimizer):\r\n",
        "\r\n",
        "    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8, weight_decay=0, degenerated_to_sgd=True):\r\n",
        "        if not 0.0 <= lr:\r\n",
        "            raise ValueError(\"Invalid learning rate: {}\".format(lr))\r\n",
        "        if not 0.0 <= eps:\r\n",
        "            raise ValueError(\"Invalid epsilon value: {}\".format(eps))\r\n",
        "        if not 0.0 <= betas[0] < 1.0:\r\n",
        "            raise ValueError(\"Invalid beta parameter at index 0: {}\".format(betas[0]))\r\n",
        "        if not 0.0 <= betas[1] < 1.0:\r\n",
        "            raise ValueError(\"Invalid beta parameter at index 1: {}\".format(betas[1]))\r\n",
        "        \r\n",
        "        self.degenerated_to_sgd = degenerated_to_sgd\r\n",
        "        if isinstance(params, (list, tuple)) and len(params) > 0 and isinstance(params[0], dict):\r\n",
        "            for param in params:\r\n",
        "                if 'betas' in param and (param['betas'][0] != betas[0] or param['betas'][1] != betas[1]):\r\n",
        "                    param['buffer'] = [[None, None, None] for _ in range(10)]\r\n",
        "        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay, buffer=[[None, None, None] for _ in range(10)])\r\n",
        "        super(RAdam, self).__init__(params, defaults)\r\n",
        "\r\n",
        "    def __setstate__(self, state):\r\n",
        "        super(RAdam, self).__setstate__(state)\r\n",
        "\r\n",
        "    def step(self, closure=None):\r\n",
        "\r\n",
        "        loss = None\r\n",
        "        if closure is not None:\r\n",
        "            loss = closure()\r\n",
        "\r\n",
        "        for group in self.param_groups:\r\n",
        "\r\n",
        "            for p in group['params']:\r\n",
        "                if p.grad is None:\r\n",
        "                    continue\r\n",
        "                grad = p.grad.data.float()\r\n",
        "                if grad.is_sparse:\r\n",
        "                    raise RuntimeError('RAdam does not support sparse gradients')\r\n",
        "\r\n",
        "                p_data_fp32 = p.data.float()\r\n",
        "\r\n",
        "                state = self.state[p]\r\n",
        "\r\n",
        "                if len(state) == 0:\r\n",
        "                    state['step'] = 0\r\n",
        "                    state['exp_avg'] = torch.zeros_like(p_data_fp32)\r\n",
        "                    state['exp_avg_sq'] = torch.zeros_like(p_data_fp32)\r\n",
        "                else:\r\n",
        "                    state['exp_avg'] = state['exp_avg'].type_as(p_data_fp32)\r\n",
        "                    state['exp_avg_sq'] = state['exp_avg_sq'].type_as(p_data_fp32)\r\n",
        "\r\n",
        "                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\r\n",
        "                beta1, beta2 = group['betas']\r\n",
        "\r\n",
        "                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\r\n",
        "                exp_avg.mul_(beta1).add_(1 - beta1, grad)\r\n",
        "\r\n",
        "                state['step'] += 1\r\n",
        "                buffered = group['buffer'][int(state['step'] % 10)]\r\n",
        "                if state['step'] == buffered[0]:\r\n",
        "                    N_sma, step_size = buffered[1], buffered[2]\r\n",
        "                else:\r\n",
        "                    buffered[0] = state['step']\r\n",
        "                    beta2_t = beta2 ** state['step']\r\n",
        "                    N_sma_max = 2 / (1 - beta2) - 1\r\n",
        "                    N_sma = N_sma_max - 2 * state['step'] * beta2_t / (1 - beta2_t)\r\n",
        "                    buffered[1] = N_sma\r\n",
        "\r\n",
        "                    # more conservative since it's an approximated value\r\n",
        "                    if N_sma >= 5:\r\n",
        "                        step_size = math.sqrt((1 - beta2_t) * (N_sma - 4) / (N_sma_max - 4) * (N_sma - 2) / N_sma * N_sma_max / (N_sma_max - 2)) / (1 - beta1 ** state['step'])\r\n",
        "                    elif self.degenerated_to_sgd:\r\n",
        "                        step_size = 1.0 / (1 - beta1 ** state['step'])\r\n",
        "                    else:\r\n",
        "                        step_size = -1\r\n",
        "                    buffered[2] = step_size\r\n",
        "\r\n",
        "                # more conservative since it's an approximated value\r\n",
        "                if N_sma >= 5:\r\n",
        "                    if group['weight_decay'] != 0:\r\n",
        "                        p_data_fp32.add_(-group['weight_decay'] * group['lr'], p_data_fp32)\r\n",
        "                    denom = exp_avg_sq.sqrt().add_(group['eps'])\r\n",
        "                    p_data_fp32.addcdiv_(-step_size * group['lr'], exp_avg, denom)\r\n",
        "                    p.data.copy_(p_data_fp32)\r\n",
        "                elif step_size > 0:\r\n",
        "                    if group['weight_decay'] != 0:\r\n",
        "                        p_data_fp32.add_(-group['weight_decay'] * group['lr'], p_data_fp32)\r\n",
        "                    p_data_fp32.add_(-step_size * group['lr'], exp_avg)\r\n",
        "                    p.data.copy_(p_data_fp32)\r\n",
        "\r\n",
        "        return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gL3XsxpzaX2x"
      },
      "source": [
        "# train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OrLGgVAi--8u"
      },
      "source": [
        "\n",
        "import time\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class TorchDataset(Dataset):\n",
        "    \"\"\"\n",
        "  Format for numpy array\n",
        "  Parameters\n",
        "  ----------\n",
        "  X : 2D array\n",
        "      The input matrix\n",
        "  y : 1D array\n",
        "      Target\n",
        "  \"\"\"\n",
        "\n",
        "    def __init__(self, x, y):\n",
        "        self.x = np.expand_dims(x, 1)\n",
        "        self.y = y\n",
        "        # self.x = x\n",
        "        # self.y = y\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.x)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        x, y = self.x[index], self.y[index]\n",
        "        return x, y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eH-Zcf_rU5WC"
      },
      "source": [
        "single version"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mdF4yBFnU8Pe"
      },
      "source": [
        "import torch_xla.distributed.parallel_loader as pl\n",
        "\n",
        "def run(params):\n",
        "    device = xm.xla_device()\n",
        "    batch_size = params[\"batch_size\"]\n",
        "    input_dim = params[\"input_dim\"]\n",
        "    output_dim = params[\"output_dim\"]\n",
        "    hidden_dim = params[\"hidden_dim\"]\n",
        "    n_layers = params[\"n_layers\"]\n",
        "    attn_heads = params[\"attn_heads\"]\n",
        "    num_epochs = params[\"num_epochs\"]\n",
        "\n",
        "\n",
        "\n",
        "    train_sampler = torch.utils.data.RandomSampler(\n",
        "    TorchDataset(X.astype(np.float32),y))\n",
        "    \n",
        "    test_sampler = torch.utils.data.RandomSampler(\n",
        "    TorchDataset(X_test.astype(np.float32),y_test))\n",
        "  \n",
        "    train_loader = torch.utils.data.DataLoader(\n",
        "      TorchDataset(X.astype(np.float32),y),\n",
        "      batch_size=params['batch_size'],\n",
        "      sampler=train_sampler)\n",
        "\n",
        "    test_loader = torch.utils.data.DataLoader(\n",
        "      TorchDataset(X_test.astype(np.float32),y_test),\n",
        "      batch_size=params['batch_size'],\n",
        "      sampler=test_sampler)\n",
        "\n",
        "\n",
        "    model = Encoder(input_dim, output_dim, hidden_dim, n_layers, attn_heads).to(device).train()\n",
        "\n",
        "    # Loss and optimizer\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, betas=(0.9, 0.999))  # Any optimizer\n",
        "    lookahead = Lookahead(optimizer, k=5, alpha=0.5)  # Initialize Lookahead\n",
        "    total_step = len(train_loader)\n",
        "\n",
        "    train_start = time.time()\n",
        "    for epoch in range(num_epochs):\n",
        "        for i, (inputs, labels) in enumerate(para_train_loader):\n",
        "            # Move tensors to the configured device\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(inputs)\n",
        "            outputs = outputs.squeeze(1)\n",
        "            loss = loss_fn(outputs, labels)\n",
        "\n",
        "            # Backward and optimize\n",
        "            lookahead.zero_grad()\n",
        "            loss.backward(retain_graph=True)  # Self-defined loss function\n",
        "            lookahead.step()\n",
        "\n",
        "            '''\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward(retain_graph=True) \n",
        "            optimizer.step()\n",
        "            '''\n",
        "            if (i + 1) % 5 == 0:\n",
        "                print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'\n",
        "                      .format(epoch + 1, num_epochs, i + 1, total_step, loss.item()))\n",
        "                with torch.no_grad():\n",
        "                    correct = 0\n",
        "                    total = 0\n",
        "                    for inputs, labels in test_loader:\n",
        "                        inputs = inputs.to(device)\n",
        "                        labels = labels.to(device)\n",
        "                        outputs = model(inputs)\n",
        "                        outputs = outputs.squeeze(1)\n",
        "                        _, predicted = torch.max(outputs.data, 1)\n",
        "                        total += labels.size(0)\n",
        "                        correct += (predicted == labels).sum().item()\n",
        "\n",
        "                    print('Accuracy: {} %'.format(100 * correct / total))\n",
        "    \n",
        "    elapsed_train_time = time.time() - train_start\n",
        "    print(\"Process\", index, \"finished training. Train time was:\", elapsed_train_time)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OF95U-wSVZQ3",
        "outputId": "ec8a3e8c-e297-417e-939f-039424ed85ae"
      },
      "source": [
        "params = {}\n",
        "params['batch_size'] = 2048 * 2\n",
        "params['num_workers'] = 8\n",
        "params['input_dim'] = 93\n",
        "params['hidden_dim'] = 256\n",
        "params['output_dim'] = 2\n",
        "params['attn_heads'] = 1\n",
        "params['dim_feedforward'] = 2048\n",
        "params['n_layers'] = 12\n",
        "params['learning_rate'] = 1e-3\n",
        "params['num_epochs'] = 2\n",
        "run(params)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:29: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "self.final_layer(x)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hhN8wVm4U1GO"
      },
      "source": [
        "multi version"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OKDNOwSydzlM"
      },
      "source": [
        "# import torch_xla.distributed.parallel_loader as pl\r\n",
        "\r\n",
        "# def map_fn(index, params):\r\n",
        "#     device = xm.xla_device()\r\n",
        "#     batch_size = params[\"batch_size\"]\r\n",
        "#     input_dim = params[\"input_dim\"]\r\n",
        "#     output_dim = params[\"output_dim\"]\r\n",
        "#     hidden_dim = params[\"hidden_dim\"]\r\n",
        "#     n_layers = params[\"n_layers\"]\r\n",
        "#     attn_heads = params[\"attn_heads\"]\r\n",
        "#     num_epochs = params[\"num_epochs\"]\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "#     train_sampler = torch.utils.data.distributed.DistributedSampler(\r\n",
        "#     TorchDataset(X.astype(np.float32),y),\r\n",
        "#     num_replicas=xm.xrt_world_size(),\r\n",
        "#     rank=xm.get_ordinal(),\r\n",
        "#     shuffle=True)\r\n",
        "    \r\n",
        "#     test_sampler = torch.utils.data.distributed.DistributedSampler(\r\n",
        "#     TorchDataset(X_test.astype(np.float32),y_test),\r\n",
        "#     num_replicas=xm.xrt_world_size(),\r\n",
        "#     rank=xm.get_ordinal(),\r\n",
        "#     shuffle=False)\r\n",
        "  \r\n",
        "#     train_loader = torch.utils.data.DataLoader(\r\n",
        "#       TorchDataset(X.astype(np.float32),y),\r\n",
        "#       batch_size=params['batch_size'],\r\n",
        "#       sampler=train_sampler,\r\n",
        "#       num_workers=params['num_workers'],\r\n",
        "#       drop_last=True)\r\n",
        "\r\n",
        "#     test_loader = torch.utils.data.DataLoader(\r\n",
        "#       TorchDataset(X_test.astype(np.float32),y_test),\r\n",
        "#       batch_size=params['batch_size'],\r\n",
        "#       sampler=test_sampler,\r\n",
        "#       num_workers=params['num_workers'],\r\n",
        "#       drop_last=True)\r\n",
        "\r\n",
        "\r\n",
        "#     model = Encoder(input_dim, output_dim, hidden_dim, n_layers, attn_heads).to(device).train()\r\n",
        "\r\n",
        "#     # Loss and optimizer\r\n",
        "#     loss_fn = nn.CrossEntropyLoss()\r\n",
        "#     optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, betas=(0.9, 0.999))  # Any optimizer\r\n",
        "#     lookahead = Lookahead(optimizer, k=5, alpha=0.5)  # Initialize Lookahead\r\n",
        "#     total_step = len(train_loader)\r\n",
        "\r\n",
        "#     train_start = time.time()\r\n",
        "#     for epoch in range(num_epochs):\r\n",
        "#         para_train_loader = pl.ParallelLoader(train_loader, [device]).per_device_loader(device)\r\n",
        "#         for i, (inputs, labels) in enumerate(para_train_loader):\r\n",
        "#             # Move tensors to the configured device\r\n",
        "#             inputs = inputs.to(device)\r\n",
        "#             labels = labels.to(device)\r\n",
        "\r\n",
        "#             # Forward pass\r\n",
        "#             outputs = model(inputs)\r\n",
        "#             print(\"outputs is \",outputs)\r\n",
        "#             print(outputs.shape)\r\n",
        "#             outputs = outputs.squeeze(1)\r\n",
        "#             loss = loss_fn(outputs, labels)\r\n",
        "\r\n",
        "#             # Backward and optimize\r\n",
        "#             lookahead.zero_grad()\r\n",
        "#             loss.backward(retain_graph=True)  # Self-defined loss function\r\n",
        "#             lookahead.step()\r\n",
        "\r\n",
        "#             '''\r\n",
        "#             optimizer.zero_grad()\r\n",
        "#             loss.backward(retain_graph=True) \r\n",
        "#             optimizer.step()\r\n",
        "#             '''\r\n",
        "#             if (i + 1) % 5 == 0:\r\n",
        "#                 print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'\r\n",
        "#                       .format(epoch + 1, num_epochs, i + 1, total_step, loss.item()))\r\n",
        "#                 with torch.no_grad():\r\n",
        "#                     correct = 0\r\n",
        "#                     total = 0\r\n",
        "#                     for inputs, labels in test_loader:\r\n",
        "#                         inputs = inputs.to(device)\r\n",
        "#                         labels = labels.to(device)\r\n",
        "#                         outputs = model(inputs)\r\n",
        "#                         outputs = outputs.squeeze(1)\r\n",
        "#                         _, predicted = torch.max(outputs.data, 1)\r\n",
        "#                         total += labels.size(0)\r\n",
        "#                         correct += (predicted == labels).sum().item()\r\n",
        "\r\n",
        "#                     print('Accuracy: {} %'.format(100 * correct / total))\r\n",
        "    \r\n",
        "#     elapsed_train_time = time.time() - train_start\r\n",
        "#     print(\"Process\", index, \"finished training. Train time was:\", elapsed_train_time)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PfC3auF7H5il"
      },
      "source": [
        "\n",
        "# import torch_xla.distributed.xla_multiprocessing as xmp\n",
        "\n",
        "# params = {}\n",
        "# params['batch_size'] = 2048 * 2\n",
        "# params['num_workers'] = 8\n",
        "# params['input_dim'] = 93\n",
        "# params['hidden_dim'] = 256\n",
        "# params['output_dim'] = 2\n",
        "# params['attn_heads'] = 1\n",
        "# params['dim_feedforward'] = 2048\n",
        "# params['n_layers'] = 12\n",
        "# params['learning_rate'] = 1e-3\n",
        "# params['num_epochs'] = 2\n",
        "\n",
        "# xmp.spawn(map_fn, args=(params,), nprocs=8, start_method='fork')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
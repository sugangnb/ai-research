{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "deep_tabnet.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sugangnb/ai-research/blob/main/colab_notebooks/alpha_effect_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CF_o9HVywlF_",
        "outputId": "910cefb9-24e2-4976-c8ad-77e368bae60d"
      },
      "source": [
        "!pip install einops"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting einops\n",
            "  Downloading https://files.pythonhosted.org/packages/5d/a0/9935e030634bf60ecd572c775f64ace82ceddf2f504a5fd3902438f07090/einops-0.3.0-py2.py3-none-any.whl\n",
            "Installing collected packages: einops\n",
            "Successfully installed einops-0.3.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SqSnpwi75tRb",
        "outputId": "7dfaf0f0-1bf6-4ca7-9e25-9d12450898bb"
      },
      "source": [
        "!pip install entmax"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting entmax\n",
            "  Downloading https://files.pythonhosted.org/packages/05/da/27fc966a4786e933778161644a1a1a228148b296d2059682799c4a8ecff8/entmax-1.0.tar.gz\n",
            "Building wheels for collected packages: entmax\n",
            "  Building wheel for entmax (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for entmax: filename=entmax-1.0-cp36-none-any.whl size=11018 sha256=b0a86d403462bb7030c5a61368b274f1c623cfbe43b95be5648843c3a873f26f\n",
            "  Stored in directory: /root/.cache/pip/wheels/4c/2c/4e/687c0abbeb16f906bd5fb8a9763e1cdd2b0d118ad55a4332f2\n",
            "Successfully built entmax\n",
            "Installing collected packages: entmax\n",
            "Successfully installed entmax-1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hVLxSbi5DuQu",
        "outputId": "d7e8f7c8-2956-4be5-fbf8-9d665cfff709"
      },
      "source": [
        "!rm -rf kicked_dataset/\r\n",
        "!git clone https://github.com/calibertytz/kicked_dataset.git"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'kicked_dataset'...\n",
            "remote: Enumerating objects: 3, done.\u001b[K\n",
            "remote: Counting objects: 100% (3/3), done.\u001b[K\n",
            "remote: Compressing objects: 100% (3/3), done.\u001b[K\n",
            "remote: Total 25 (delta 0), reused 0 (delta 0), pack-reused 22\u001b[K\n",
            "Unpacking objects: 100% (25/25), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ct0uA3PcDw1o"
      },
      "source": [
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "\r\n",
        "df_train = pd.read_csv('kicked_dataset/df_train_final.csv')\r\n",
        "df_test = pd.read_csv('kicked_dataset/df_test_final.csv')\r\n",
        "\r\n",
        "train_X = df_train.drop(columns=['label'])\r\n",
        "train_Y = df_train['label']\r\n",
        "\r\n",
        "X_test = df_test.drop(columns=['label'])\r\n",
        "Y_test = df_test['label']\r\n",
        "\r\n",
        "test_X, val_X, test_Y, val_Y = train_test_split(X_test, Y_test, test_size=0.5, random_state=1) # x_val for updating alpha"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zl83DRLXrhQ-"
      },
      "source": [
        "def cate_count(df):\r\n",
        "  count_res = df.nunique()\r\n",
        "  return count_res.values"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hd-dxmqYrJg9"
      },
      "source": [
        "num_tokens = cate_count(train_X)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s4bcQdRbhEiP"
      },
      "source": [
        "train_X = train_X.values\r\n",
        "test_X = test_X.values\r\n",
        "val_X = val_X.values\r\n",
        "\r\n",
        "train_Y = train_Y.values\r\n",
        "test_Y = test_Y.values\r\n",
        "val_Y = val_Y.values\r\n",
        "\r\n",
        "#train_Y = pd.get_dummies(train_Y).values\r\n",
        "#test_Y = pd.get_dummies(test_Y).values\r\n",
        "#val_Y = pd.get_dummies(val_Y).values"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Unx2HE7yhXWd",
        "outputId": "7212bdb3-d135-442a-9c27-9bab6497a650"
      },
      "source": [
        "train_X.shape, train_Y.shape"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((50000, 93), (50000,))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C2kYQDSOxLyN"
      },
      "source": [
        "import torch\r\n",
        "import torch.nn.functional as F\r\n",
        "from torch import nn, einsum\r\n",
        "from einops import rearrange\r\n",
        "from entmax import sparsemax, entmax15, entmax_bisect\r\n",
        "\r\n",
        "\r\n",
        "# helpers\r\n",
        "\r\n",
        "def exists(val):\r\n",
        "    return val is not None\r\n",
        "\r\n",
        "\r\n",
        "def default(val, d):\r\n",
        "    return val if exists(val) else d\r\n",
        "\r\n",
        "\r\n",
        "# classes\r\n",
        "\r\n",
        "class Residual(nn.Module):\r\n",
        "    def __init__(self, fn):\r\n",
        "        super().__init__()\r\n",
        "        self.fn = fn\r\n",
        "\r\n",
        "    def forward(self, x, **kwargs):\r\n",
        "        return self.fn(x, **kwargs) + x\r\n",
        "\r\n",
        "\r\n",
        "class PreNorm(nn.Module):\r\n",
        "    def __init__(self, dim, fn):\r\n",
        "        super().__init__()\r\n",
        "        self.norm = nn.LayerNorm(dim)\r\n",
        "        self.fn = fn\r\n",
        "\r\n",
        "    def forward(self, x, **kwargs):\r\n",
        "        return self.fn(self.norm(x), **kwargs)\r\n",
        "\r\n",
        "\r\n",
        "class LayerNorm(nn.Module):\r\n",
        "    \"Construct a layernorm module (See citation for details).\"\r\n",
        "\r\n",
        "    def __init__(self, features, eps=1e-12):\r\n",
        "        super(LayerNorm, self).__init__()\r\n",
        "        self.a_2 = nn.Parameter(torch.ones(features))\r\n",
        "        self.b_2 = nn.Parameter(torch.zeros(features))\r\n",
        "        self.eps = eps\r\n",
        "\r\n",
        "    def forward(self, x):\r\n",
        "        mean = x.mean(-1, keepdim=True)\r\n",
        "        std = x.std(-1, keepdim=True)\r\n",
        "        return self.a_2 * (x - mean) / (std + self.eps) + self.b_2\r\n",
        "\r\n",
        "\r\n",
        "# attention\r\n",
        "\r\n",
        "class GEGLU(nn.Module):\r\n",
        "    def forward(self, x):\r\n",
        "        x, gates = x.chunk(2, dim=-1)\r\n",
        "        return x * F.gelu(gates)\r\n",
        "\r\n",
        "\r\n",
        "class FeedForward(nn.Module):\r\n",
        "    def __init__(self, dim, mult=4, dropout=0.1):\r\n",
        "        super().__init__()\r\n",
        "        self.net = nn.Sequential(\r\n",
        "            nn.Linear(dim, dim * mult * 2),\r\n",
        "            GEGLU(),\r\n",
        "            nn.Dropout(dropout),\r\n",
        "            nn.Linear(dim * mult, dim)\r\n",
        "        )\r\n",
        "\r\n",
        "    def forward(self, x, **kwargs):\r\n",
        "        return self.net(x)\r\n",
        "\r\n",
        "\r\n",
        "class Attention(nn.Module):\r\n",
        "    def __init__(\r\n",
        "            self,\r\n",
        "            dim,\r\n",
        "            heads=8,\r\n",
        "            dim_head=16,\r\n",
        "            dropout=0.1,\r\n",
        "            mask_type='entmax15'\r\n",
        "    ):\r\n",
        "        super().__init__()\r\n",
        "        inner_dim = dim_head * heads\r\n",
        "        self.heads = heads\r\n",
        "        self.mask_type = mask_type\r\n",
        "        self.scale = dim_head ** -0.5\r\n",
        "\r\n",
        "        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias=False)\r\n",
        "        self.to_out = nn.Linear(inner_dim, dim)\r\n",
        "\r\n",
        "        self.dropout = nn.Dropout(dropout)\r\n",
        "\r\n",
        "    def forward(self, x):\r\n",
        "        h = self.heads\r\n",
        "        q, k, v = self.to_qkv(x).chunk(3, dim=-1)\r\n",
        "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h=h), (q, k, v))\r\n",
        "        sim = einsum('b h i d, b h j d -> b h i j', q, k) * self.scale\r\n",
        "\r\n",
        "        if self.mask_type == 'entmax15':\r\n",
        "            attn = entmax15(sim, dim=-1)\r\n",
        "        elif self.mask_type == 'sparsemax':\r\n",
        "            attn = sparsemax(sim, dim=-1)\r\n",
        "        elif self.mask_type == 'entmax':\r\n",
        "            alpha = 1.5\r\n",
        "            attn = sim * entmax_bisect(sim, alpha=alpha, dim=-1)\r\n",
        "        else:\r\n",
        "            raise NotImplemented()\r\n",
        "        attn = self.dropout(attn)\r\n",
        "        out = einsum('b h i j, b h j d -> b h i d', attn, v)\r\n",
        "        out = rearrange(out, 'b h n d -> b n (h d)', h=h)\r\n",
        "        return self.to_out(out)\r\n",
        "\r\n",
        "\r\n",
        "# transformer\r\n",
        "\r\n",
        "class Transformer(nn.Module):\r\n",
        "    def __init__(self, dim, heads=8, dim_head=16, attn_dropout=0.1, ff_dropout=0.1, depth=1):\r\n",
        "        super().__init__()\r\n",
        "        self.layers = nn.ModuleList([])\r\n",
        "        self.depth = depth\r\n",
        "\r\n",
        "        for _ in range(self.depth):\r\n",
        "            self.layers.append(nn.ModuleList([\r\n",
        "                Residual(PreNorm(dim, Attention(dim, heads=heads, dim_head=dim_head, dropout=attn_dropout,mask_type=\"entmax\"))),\r\n",
        "                Residual(PreNorm(dim, FeedForward(dim, dropout=ff_dropout))),\r\n",
        "            ]))\r\n",
        "\r\n",
        "    def forward(self, x):\r\n",
        "        for i, (attn, ff) in enumerate(self.layers):\r\n",
        "            x = attn(x)\r\n",
        "            x = ff(x)\r\n",
        "        return x\r\n",
        "\r\n",
        "\r\n",
        "class DeepTabnet(nn.Module):\r\n",
        "    def __init__(\r\n",
        "            self,\r\n",
        "            categories,\r\n",
        "            dim,\r\n",
        "            depth_info,\r\n",
        "            heads=8,\r\n",
        "            dim_head=16,\r\n",
        "            dim_out=2,\r\n",
        "            attn_dropout=0.1,\r\n",
        "            ff_dropout=0.1\r\n",
        "    ):\r\n",
        "        super().__init__()\r\n",
        "        assert all(map(lambda n: n > 0, categories)), 'number of each category must be positive'\r\n",
        "\r\n",
        "        # categories related calculations\r\n",
        "\r\n",
        "        self.num_categories = len(categories)\r\n",
        "        self.num_unique_categories = sum(categories)\r\n",
        "        total_tokens = self.num_unique_categories\r\n",
        "\r\n",
        "        # embedding\r\n",
        "        self.embeds = nn.Embedding(total_tokens, dim)\r\n",
        "\r\n",
        "        # transformer\r\n",
        "        self.transformer_base = Transformer(\r\n",
        "            dim=dim,\r\n",
        "            depth=depth_info[0],\r\n",
        "            heads=heads,\r\n",
        "            dim_head=dim_head,\r\n",
        "            attn_dropout=attn_dropout,\r\n",
        "            ff_dropout=ff_dropout\r\n",
        "        )\r\n",
        "        self.no_shared_transformer = nn.ModuleList()\r\n",
        "        for i in range(1, len(depth_info)):\r\n",
        "            trans = Transformer(\r\n",
        "                dim=dim,\r\n",
        "                depth=depth_info[i],\r\n",
        "                heads=heads,\r\n",
        "                dim_head=dim_head,\r\n",
        "                attn_dropout=attn_dropout,\r\n",
        "                ff_dropout=ff_dropout\r\n",
        "            )\r\n",
        "            self.no_shared_transformer.append(trans)\r\n",
        "        # final layer\r\n",
        "        self.mlp = nn.Sequential(nn.ReLU(),\r\n",
        "                                 nn.Linear(dim * 93, dim_out))\r\n",
        "\r\n",
        "    def forward(self, x_categ):\r\n",
        "        assert x_categ.shape[\r\n",
        "                   -1] == self.num_categories, f'you must pass in {self.num_categories} values for your categories input'\r\n",
        "        x_categ_embed = self.embeds(x_categ)\r\n",
        "\r\n",
        "        for t in self.no_shared_transformer:\r\n",
        "            x = self.transformer_base(x_categ_embed)\r\n",
        "            x = t(x)\r\n",
        "        flat_categ = x.flatten(1)\r\n",
        "        return self.mlp(flat_categ)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9S44IU8rDyWu"
      },
      "source": [
        "from torch.utils.data import Dataset, DataLoader\r\n",
        "import torch\r\n",
        "import numpy as np\r\n",
        "\r\n",
        "\r\n",
        "class TorchDataset(Dataset):\r\n",
        "    \"\"\"\r\n",
        "    Format for numpy array\r\n",
        "    Parameters\r\n",
        "    ----------\r\n",
        "    X : 2D array\r\n",
        "        The input matrix\r\n",
        "    y : 1D array\r\n",
        "        Target\r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    def __init__(self, x, y):\r\n",
        "        self.x = x\r\n",
        "        self.y = y\r\n",
        "\r\n",
        "    def __len__(self):\r\n",
        "        return len(self.x)\r\n",
        "\r\n",
        "    def __getitem__(self, index):\r\n",
        "        x, y = self.x[index], self.y[index]\r\n",
        "        return x, y\r\n",
        "\r\n",
        "# Device configuration\r\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\r\n",
        "\r\n",
        "\r\n",
        "categories = num_tokens\r\n",
        "dim = 64\r\n",
        "depth_info = (1, 2, 2, 2, 2)\r\n",
        "batch_size = 256\r\n",
        "learning_rate = 1e-3\r\n",
        "num_epochs = 2\r\n",
        "\r\n",
        "train_loader = DataLoader(TorchDataset(train_X.astype(np.float32), train_Y),\r\n",
        "        batch_size=batch_size)\r\n",
        "\r\n",
        "val_loader = DataLoader(TorchDataset(test_X.astype(np.float32), test_Y),\r\n",
        "        batch_size=batch_size)\r\n",
        "        \r\n",
        "test_loader = DataLoader(TorchDataset(val_X.astype(np.float32), val_Y),\r\n",
        "        batch_size=batch_size)\r\n",
        "\r\n",
        "model = DeepTabnet(categories, dim, depth_info).to(device)\r\n",
        "\r\n",
        "# Loss and optimizer\r\n",
        "criterion = nn.CrossEntropyLoss()\r\n",
        "\r\n",
        "# add radam and lookahead (TODO)\r\n",
        "opt_1 = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=learning_rate)\r\n",
        "\r\n",
        "\r\n",
        "def train():\r\n",
        "  # Train the model\r\n",
        "  total_step = len(train_loader)\r\n",
        "  for epoch in range(num_epochs):\r\n",
        "      print('train model param \\n')\r\n",
        "      for i, (inputs, labels) in enumerate(train_loader):  \r\n",
        "          # Move tensors to the configured device\r\n",
        "          inputs = inputs.to(device=device, dtype=torch.long)\r\n",
        "          labels = labels.to(device)\r\n",
        "          # Forward pass\r\n",
        "          outputs = model(inputs)\r\n",
        "          loss1 = criterion(outputs, labels)\r\n",
        "          \r\n",
        "          # Backward and optimize\r\n",
        "          opt_1.zero_grad()\r\n",
        "          #loss1.backward(retain_graph=True)\r\n",
        "          loss1.backward() \r\n",
        "          opt_1.step()\r\n",
        "          if (i+1) % 5 == 0:\r\n",
        "              print ('train mode, Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \r\n",
        "                    .format(epoch+1, num_epochs, i+1, total_step, loss1.item()))\r\n",
        "              val_train()\r\n",
        "              val()\r\n",
        "              \r\n",
        "def val_train():\r\n",
        "    with torch.no_grad():\r\n",
        "      correct = 0\r\n",
        "      total = 0\r\n",
        "      for inputs, labels in train_loader:\r\n",
        "          inputs = inputs.to(device=device, dtype=torch.long)\r\n",
        "          labels = labels.to(device)\r\n",
        "          outputs = model(inputs)\r\n",
        "          _, predicted = torch.max(outputs.data, 1)\r\n",
        "          total += labels.size(0)\r\n",
        "          correct += (predicted == labels).sum().item()\r\n",
        "\r\n",
        "      print('train_accuracy: {} %'.format(100 * correct / total))\r\n",
        "\r\n",
        "\r\n",
        "def val():\r\n",
        "  # Test the model\r\n",
        "  # In test phase, we don't need to compute gradients (for memory efficiency)\r\n",
        "  with torch.no_grad():\r\n",
        "      correct = 0\r\n",
        "      total = 0\r\n",
        "      for inputs, labels in test_loader:\r\n",
        "          inputs = inputs.to(device=device, dtype=torch.long)\r\n",
        "          labels = labels.to(device)\r\n",
        "          outputs = model(inputs)\r\n",
        "          _, predicted = torch.max(outputs.data, 1)\r\n",
        "          total += labels.size(0)\r\n",
        "          correct += (predicted == labels).sum().item()\r\n",
        "\r\n",
        "      print('Accuracy: {} %'.format(100 * correct / total))"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sPsqn0FkD1mp",
        "outputId": "4eccc4f3-02a6-45a0-9544-b3d9dbf433e4"
      },
      "source": [
        "train()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train model param \n",
            "\n",
            "train mode, Epoch [1/2], Step [5/196], Loss: 0.3893\n",
            "train_accuracy: 87.27 %\n",
            "Accuracy: 85.99895579533589 %\n",
            "train mode, Epoch [1/2], Step [10/196], Loss: 0.3310\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Fn9CvZRu9B4"
      },
      "source": [
        "class Attention(nn.Module):\n",
        "    def __init__(\n",
        "            self,\n",
        "            dim,\n",
        "            heads=8,\n",
        "            dim_head=16,\n",
        "            dropout=0.1,\n",
        "            mask_type='entmax15'\n",
        "    ):\n",
        "        super().__init__()\n",
        "        inner_dim = dim_head * heads\n",
        "        self.heads = heads\n",
        "        self.mask_type = mask_type\n",
        "        self.scale = dim_head ** -0.5\n",
        "\n",
        "        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias=False)\n",
        "        self.to_out = nn.Linear(inner_dim, dim)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h = self.heads\n",
        "        q, k, v = self.to_qkv(x).chunk(3, dim=-1)\n",
        "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h=h), (q, k, v))\n",
        "        sim = einsum('b h i d, b h j d -> b h i j', q, k) * self.scale\n",
        "\n",
        "        if self.mask_type == 'entmax15':\n",
        "            attn = entmax15(sim, dim=-1)\n",
        "        elif self.mask_type == 'sparsemax':\n",
        "            attn = sparsemax(sim, dim=-1)\n",
        "        elif self.mask_type == 'entmax':\n",
        "            alpha = 1\n",
        "            attn = sim * entmax_bisect(sim, alpha=alpha, dim=-1)\n",
        "        else:\n",
        "            raise NotImplemented()\n",
        "        attn = self.dropout(attn)\n",
        "        out = einsum('b h i j, b h j d -> b h i d', attn, v)\n",
        "        out = rearrange(out, 'b h n d -> b n (h d)', h=h)\n",
        "        return self.to_out(out)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "smArRvwIwHm8"
      },
      "source": [
        "train()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mTUtAzWFxyxJ"
      },
      "source": [
        "class Attention(nn.Module):\n",
        "    def __init__(\n",
        "            self,\n",
        "            dim,\n",
        "            heads=8,\n",
        "            dim_head=16,\n",
        "            dropout=0.1,\n",
        "            mask_type='entmax15'\n",
        "    ):\n",
        "        super().__init__()\n",
        "        inner_dim = dim_head * heads\n",
        "        self.heads = heads\n",
        "        self.mask_type = mask_type\n",
        "        self.scale = dim_head ** -0.5\n",
        "\n",
        "        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias=False)\n",
        "        self.to_out = nn.Linear(inner_dim, dim)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h = self.heads\n",
        "        q, k, v = self.to_qkv(x).chunk(3, dim=-1)\n",
        "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h=h), (q, k, v))\n",
        "        sim = einsum('b h i d, b h j d -> b h i j', q, k) * self.scale\n",
        "\n",
        "        if self.mask_type == 'entmax15':\n",
        "            attn = entmax15(sim, dim=-1)\n",
        "        elif self.mask_type == 'sparsemax':\n",
        "            attn = sparsemax(sim, dim=-1)\n",
        "        elif self.mask_type == 'entmax':\n",
        "            alpha = 2\n",
        "            attn = sim * entmax_bisect(sim, alpha=alpha, dim=-1)\n",
        "        else:\n",
        "            raise NotImplemented()\n",
        "        attn = self.dropout(attn)\n",
        "        out = einsum('b h i j, b h j d -> b h i d', attn, v)\n",
        "        out = rearrange(out, 'b h n d -> b n (h d)', h=h)\n",
        "        return self.to_out(out)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x6E977aGypQL"
      },
      "source": [
        "train()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ml5BF3le3Fs3"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}